#!/usr/bin/env python3
"""
ê°ì • ê¸°ë°˜ ë””ì§€í„¸ ì¹˜ë£Œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œ
- SD-1.5 ê¸°ë°˜ ê²½ëŸ‰í™” ì´ë¯¸ì§€ ìƒì„±
- VAD ëª¨ë¸ ê¸°ë°˜ ì™„ë²½í•œ ê°ì • ë¶„ì„
- LoRA ê°œì¸í™” ì–´ëŒ‘í„°
- DRaFT+ ê°•í™”í•™ìŠµ
- CLI ê¸°ë°˜ í„°ë¯¸ë„ ì¸í„°í˜ì´ìŠ¤
"""

import os
import sys
import json
import argparse
import warnings
import sqlite3
import pickle
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
import math
import random

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# Apple ìµœì í™” ì„¤ì •
if torch.backends.mps.is_available():
    device_type = "mps"
    torch.mps.set_per_process_memory_fraction(0.8)
elif torch.cuda.is_available():
    device_type = "cuda"
    torch.backends.cudnn.benchmark = True
else:
    device_type = "cpu"

device = torch.device(device_type)
print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("emotion_therapy.log"),
        logging.StreamHandler(sys.stdout),
    ],
)
logger = logging.getLogger(__name__)

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë™ì  ì„í¬íŠ¸
try:
    from transformers import (
        CLIPTextModel,
        CLIPTokenizer,
        AutoTokenizer,
        AutoModel,
        pipeline,
        RobertaTokenizer,
        RobertaModel,
    )

    TRANSFORMERS_AVAILABLE = True
    logger.info("âœ… Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.error("âŒ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install transformers")
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import (
        StableDiffusionPipeline,
        UNet2DConditionModel,
        DDPMScheduler,
        AutoencoderKL,
        DPMSolverMultistepScheduler,
        EulerDiscreteScheduler,
    )

    DIFFUSERS_AVAILABLE = True
    logger.info("âœ… Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.error("âŒ diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install diffusers")
    DIFFUSERS_AVAILABLE = False

try:
    from peft import LoraConfig, get_peft_model, TaskType, PeftModel

    PEFT_AVAILABLE = True
    logger.info("âœ… PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.error("âŒ peft ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install peft")
    PEFT_AVAILABLE = False

# =============================================================================
# ê°ì • ì„ë² ë”© ë° VAD ëª¨ë¸
# =============================================================================


@dataclass
class EmotionEmbedding:
    """Valence-Arousal-Dominance ê¸°ë°˜ ê°ì • ì„ë² ë”©"""

    valence: float  # -1.0 (ë¶€ì •) to 1.0 (ê¸ì •)
    arousal: float  # -1.0 (ì°¨ë¶„) to 1.0 (í¥ë¶„)
    dominance: float = 0.0  # -1.0 (ìˆ˜ë™) to 1.0 (ì§€ë°°ì )
    confidence: float = 1.0  # ê°ì • ì˜ˆì¸¡ ì‹ ë¢°ë„

    def to_vector(self) -> np.ndarray:
        return np.array([self.valence, self.arousal, self.dominance])

    def to_dict(self) -> Dict[str, float]:
        return asdict(self)

    @classmethod
    def from_vector(cls, vector: np.ndarray, confidence: float = 1.0):
        return cls(
            valence=float(vector[0]),
            arousal=float(vector[1]),
            dominance=float(vector[2]) if len(vector) > 2 else 0.0,
            confidence=confidence,
        )

    def distance_to(self, other: "EmotionEmbedding") -> float:
        """ë‹¤ë¥¸ ê°ì •ê³¼ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬"""
        return np.linalg.norm(self.to_vector() - other.to_vector())

    def similarity_to(self, other: "EmotionEmbedding") -> float:
        """ë‹¤ë¥¸ ê°ì •ê³¼ì˜ ìœ ì‚¬ë„ (0-1)"""
        max_distance = np.sqrt(3 * 4)  # ìµœëŒ€ ê±°ë¦¬ (ê° ì°¨ì› -2 to 2)
        return 1.0 - (self.distance_to(other) / max_distance)


class AdvancedEmotionMapper:
    """ê³ ê¸‰ VAD ê¸°ë°˜ ê°ì • ë§¤í•‘ ì‹œìŠ¤í…œ"""

    def __init__(self, model_name="klue/roberta-large"):
        self.device = device
        self.model_name = model_name

        # ê°ì • ì–´íœ˜ ì‚¬ì „ (í•œêµ­ì–´ + ì˜ì–´)
        self.emotion_lexicon = {
            # ê¸°ë³¸ ê°ì •ë“¤
            "ê¸°ì¨": EmotionEmbedding(0.8, 0.6, 0.4),
            "í–‰ë³µ": EmotionEmbedding(0.8, 0.5, 0.3),
            "ì¦ê±°ì›€": EmotionEmbedding(0.7, 0.7, 0.4),
            "ì‹ ë‚¨": EmotionEmbedding(0.9, 0.8, 0.6),
            "ë§Œì¡±": EmotionEmbedding(0.6, 0.2, 0.3),
            "ë¿Œë“¯": EmotionEmbedding(0.7, 0.4, 0.7),
            "ìŠ¬í””": EmotionEmbedding(-0.7, -0.3, -0.5),
            "ìš°ìš¸": EmotionEmbedding(-0.8, -0.4, -0.6),
            "í—ˆë¬´": EmotionEmbedding(-0.5, -0.6, -0.4),
            "ì ˆë§": EmotionEmbedding(-0.9, 0.3, -0.8),
            "ìƒì‹¤": EmotionEmbedding(-0.8, -0.2, -0.6),
            "ì™¸ë¡œì›€": EmotionEmbedding(-0.6, -0.2, -0.6),
            "í™”ë‚¨": EmotionEmbedding(-0.6, 0.8, 0.7),
            "ë¶„ë…¸": EmotionEmbedding(-0.8, 0.9, 0.8),
            "ì§œì¦": EmotionEmbedding(-0.5, 0.6, 0.4),
            "ë‹µë‹µ": EmotionEmbedding(-0.4, 0.5, -0.2),
            "ì–µìš¸": EmotionEmbedding(-0.7, 0.6, -0.3),
            "ë‘ë ¤ì›€": EmotionEmbedding(-0.8, 0.7, -0.8),
            "ê±±ì •": EmotionEmbedding(-0.5, 0.6, -0.4),
            "ë¶ˆì•ˆ": EmotionEmbedding(-0.5, 0.7, -0.5),
            "ë¬´ì„œì›€": EmotionEmbedding(-0.8, 0.8, -0.7),
            "ê¸´ì¥": EmotionEmbedding(-0.2, 0.8, -0.3),
            "ë†€ëŒ": EmotionEmbedding(0.2, 0.9, 0.1),
            "ë‹¹í™©": EmotionEmbedding(-0.2, 0.8, -0.4),
            "ì¶©ê²©": EmotionEmbedding(-0.3, 0.9, -0.2),
            "í‰ì˜¨": EmotionEmbedding(0.4, -0.7, 0.2),
            "ì°¨ë¶„": EmotionEmbedding(0.3, -0.8, 0.1),
            "í¸ì•ˆ": EmotionEmbedding(0.6, -0.5, 0.3),
            "ì•ˆì •": EmotionEmbedding(0.5, -0.6, 0.4),
            "ìŠ¤íŠ¸ë ˆìŠ¤": EmotionEmbedding(-0.6, 0.7, -0.3),
            "í”¼ê³¤": EmotionEmbedding(-0.3, -0.8, -0.4),
            "ì§€ì¹¨": EmotionEmbedding(-0.4, -0.7, -0.5),
            "ê¶Œíƒœ": EmotionEmbedding(-0.2, -0.8, -0.3),
            "ì‚¬ë‘": EmotionEmbedding(0.9, 0.5, 0.3),
            "ì• ì •": EmotionEmbedding(0.8, 0.4, 0.4),
            "ê·¸ë¦¬ì›€": EmotionEmbedding(0.3, 0.3, -0.2),
            "ê°ì‚¬": EmotionEmbedding(0.8, 0.3, 0.3),
            "ê³ ë§ˆì›€": EmotionEmbedding(0.7, 0.2, 0.2),
            # ì˜ì–´ ê°ì •ë“¤
            "joy": EmotionEmbedding(0.8, 0.6, 0.4),
            "happiness": EmotionEmbedding(0.8, 0.5, 0.3),
            "sadness": EmotionEmbedding(-0.7, -0.3, -0.5),
            "anger": EmotionEmbedding(-0.6, 0.8, 0.7),
            "fear": EmotionEmbedding(-0.8, 0.7, -0.8),
            "surprise": EmotionEmbedding(0.2, 0.9, 0.1),
            "love": EmotionEmbedding(0.9, 0.5, 0.3),
            "peace": EmotionEmbedding(0.4, -0.7, 0.2),
            "stress": EmotionEmbedding(-0.6, 0.7, -0.3),
            "tired": EmotionEmbedding(-0.3, -0.8, -0.4),
        }

        # ê°ì • ê°•í™” í‘œí˜„ë“¤
        self.emotion_intensifiers = {
            "ë§¤ìš°": 1.3,
            "ì •ë§": 1.2,
            "ì—„ì²­": 1.4,
            "ë„ˆë¬´": 1.3,
            "ì™„ì „": 1.4,
            "ì¡°ê¸ˆ": 0.7,
            "ì•½ê°„": 0.6,
            "ì‚´ì§": 0.5,
            "ì¢€": 0.7,
            "extremely": 1.4,
            "very": 1.3,
            "really": 1.2,
            "quite": 1.1,
            "slightly": 0.6,
            "somewhat": 0.7,
            "a bit": 0.6,
        }

        # ë¶€ì • í‘œí˜„ë“¤
        self.negation_words = {
            "ì•ˆ",
            "ëª»",
            "ì—†",
            "ì•„ë‹ˆ",
            "not",
            "no",
            "never",
            "don't",
            "can't",
            "won't",
        }

        # Transformer ëª¨ë¸ ë¡œë“œ
        self.use_transformer = TRANSFORMERS_AVAILABLE
        if self.use_transformer:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.text_encoder = AutoModel.from_pretrained(model_name).to(
                    self.device
                )
                self.text_encoder.eval()

                # VAD ì˜ˆì¸¡ í—¤ë“œ
                hidden_size = self.text_encoder.config.hidden_size
                self.vad_predictor = nn.Sequential(
                    nn.Linear(hidden_size, 512),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(256, 128),
                    nn.ReLU(),
                    nn.Linear(128, 3),  # valence, arousal, dominance
                    nn.Tanh(),
                ).to(self.device)

                # ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
                self._init_vad_predictor()

                logger.info(f"âœ… ê³ ê¸‰ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            except Exception as e:
                logger.warning(
                    f"âš ï¸ Transformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}, ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œ ì‚¬ìš©"
                )
                self.use_transformer = False

    def _init_vad_predictor(self):
        """VAD ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”"""
        for module in self.vad_predictor:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.zeros_(module.bias)

    def extract_emotion_from_text(self, text: str) -> EmotionEmbedding:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°ì • ì¶”ì¶œ (ë‹¤ì¤‘ ë°©ë²•ë¡  ìœµí•©)"""
        # 1. ê·œì¹™ ê¸°ë°˜ ê°ì • ë¶„ì„
        rule_based_emotion = self._rule_based_emotion_analysis(text)

        # 2. Transformer ê¸°ë°˜ ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.use_transformer:
            try:
                transformer_emotion = self._transformer_emotion_analysis(text)
                # ë‘ ê²°ê³¼ë¥¼ ê°€ì¤‘ í‰ê· 
                final_emotion = self._combine_emotions(
                    rule_based_emotion,
                    transformer_emotion,
                    rule_weight=0.4,
                    transformer_weight=0.6,
                )
            except Exception as e:
                logger.warning(f"Transformer ë¶„ì„ ì‹¤íŒ¨: {e}, ê·œì¹™ ê¸°ë°˜ ê²°ê³¼ ì‚¬ìš©")
                final_emotion = rule_based_emotion
        else:
            final_emotion = rule_based_emotion

        # 3. í›„ì²˜ë¦¬ ë° ì •ê·œí™”
        final_emotion = self._post_process_emotion(final_emotion, text)

        logger.info(
            f"ê°ì • ë¶„ì„ ê²°ê³¼: V={final_emotion.valence:.3f}, A={final_emotion.arousal:.3f}, D={final_emotion.dominance:.3f}"
        )
        return final_emotion

    def _rule_based_emotion_analysis(self, text: str) -> EmotionEmbedding:
        """ê·œì¹™ ê¸°ë°˜ ê°ì • ë¶„ì„"""
        text_lower = text.lower()
        words = text_lower.split()

        detected_emotions = []
        emotion_weights = []

        # ê°ì • ë‹¨ì–´ íƒì§€
        for i, word in enumerate(words):
            # ê°ì • ì–´íœ˜ ë§¤ì¹­
            for emotion_word, emotion_emb in self.emotion_lexicon.items():
                if emotion_word in word or word in emotion_word:
                    # ê°•í™” í‘œí˜„ ì²´í¬
                    intensity = 1.0
                    if i > 0 and words[i - 1] in self.emotion_intensifiers:
                        intensity = self.emotion_intensifiers[words[i - 1]]

                    # ë¶€ì • í‘œí˜„ ì²´í¬
                    negated = False
                    for j in range(max(0, i - 2), i):
                        if words[j] in self.negation_words:
                            negated = True
                            break

                    # ê°ì • ì„ë² ë”© ì¡°ì •
                    adjusted_emotion = EmotionEmbedding(
                        valence=emotion_emb.valence
                        * intensity
                        * (-1 if negated else 1),
                        arousal=emotion_emb.arousal * intensity,
                        dominance=emotion_emb.dominance
                        * intensity
                        * (-1 if negated else 1),
                    )

                    detected_emotions.append(adjusted_emotion)
                    emotion_weights.append(intensity)

        if not detected_emotions:
            # ê¸°ë³¸ ì¤‘ì„± ê°ì •
            return EmotionEmbedding(0.0, 0.0, 0.0, confidence=0.3)

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_weight = sum(emotion_weights)
        avg_valence = (
            sum(e.valence * w for e, w in zip(detected_emotions, emotion_weights))
            / total_weight
        )
        avg_arousal = (
            sum(e.arousal * w for e, w in zip(detected_emotions, emotion_weights))
            / total_weight
        )
        avg_dominance = (
            sum(e.dominance * w for e, w in zip(detected_emotions, emotion_weights))
            / total_weight
        )

        confidence = min(
            1.0, len(detected_emotions) / 3.0
        )  # ê°ì • ë‹¨ì–´ ê°œìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„

        return EmotionEmbedding(avg_valence, avg_arousal, avg_dominance, confidence)

    def _transformer_emotion_analysis(self, text: str) -> EmotionEmbedding:
        """Transformer ê¸°ë°˜ ê°ì • ë¶„ì„"""
        if not self.use_transformer:
            return EmotionEmbedding(0.0, 0.0, 0.0, confidence=0.0)

        # í† í°í™” ë° ì¸ì½”ë”©
        inputs = self.tokenizer(
            text, return_tensors="pt", max_length=512, truncation=True, padding=True
        ).to(self.device)

        with torch.no_grad():
            # í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ
            outputs = self.text_encoder(**inputs)
            # CLS í† í° ë˜ëŠ” í‰ê·  í’€ë§ ì‚¬ìš©
            if hasattr(outputs, "pooler_output") and outputs.pooler_output is not None:
                text_features = outputs.pooler_output
            else:
                text_features = outputs.last_hidden_state.mean(dim=1)

            # VAD ì˜ˆì¸¡
            vad_scores = self.vad_predictor(text_features)
            vad_scores = vad_scores.squeeze().cpu().numpy()

        return EmotionEmbedding(
            valence=float(vad_scores[0]),
            arousal=float(vad_scores[1]),
            dominance=float(vad_scores[2]),
            confidence=0.8,
        )

    def _combine_emotions(
        self,
        emotion1: EmotionEmbedding,
        emotion2: EmotionEmbedding,
        rule_weight: float = 0.5,
        transformer_weight: float = 0.5,
    ) -> EmotionEmbedding:
        """ë‘ ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ê²°í•©"""
        total_weight = rule_weight + transformer_weight
        rule_weight /= total_weight
        transformer_weight /= total_weight

        # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
        rule_conf_weight = rule_weight * emotion1.confidence
        trans_conf_weight = transformer_weight * emotion2.confidence
        total_conf_weight = rule_conf_weight + trans_conf_weight

        if total_conf_weight > 0:
            rule_conf_weight /= total_conf_weight
            trans_conf_weight /= total_conf_weight
        else:
            rule_conf_weight = trans_conf_weight = 0.5

        return EmotionEmbedding(
            valence=emotion1.valence * rule_conf_weight
            + emotion2.valence * trans_conf_weight,
            arousal=emotion1.arousal * rule_conf_weight
            + emotion2.arousal * trans_conf_weight,
            dominance=emotion1.dominance * rule_conf_weight
            + emotion2.dominance * trans_conf_weight,
            confidence=min(1.0, emotion1.confidence + emotion2.confidence),
        )

    def _post_process_emotion(
        self, emotion: EmotionEmbedding, text: str
    ) -> EmotionEmbedding:
        """ê°ì • í›„ì²˜ë¦¬ ë° ì •ê·œí™”"""
        # ë²”ìœ„ ì œí•œ
        valence = np.clip(emotion.valence, -1.0, 1.0)
        arousal = np.clip(emotion.arousal, -1.0, 1.0)
        dominance = np.clip(emotion.dominance, -1.0, 1.0)

        # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ì‹ ë¢°ë„ ì¡°ì •
        text_length_factor = min(1.0, len(text.split()) / 10.0)
        confidence = emotion.confidence * text_length_factor

        return EmotionEmbedding(valence, arousal, dominance, confidence)

    def emotion_to_prompt_modifiers(self, emotion: EmotionEmbedding) -> str:
        """ê°ì •ì„ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ìë¡œ ë³€í™˜"""
        modifiers = []

        # Valence ê¸°ë°˜ ìˆ˜ì •ì
        if emotion.valence > 0.6:
            modifiers.extend(["bright", "cheerful", "uplifting", "positive"])
        elif emotion.valence > 0.2:
            modifiers.extend(["pleasant", "mild", "gentle"])
        elif emotion.valence < -0.6:
            modifiers.extend(["dark", "melancholic", "somber", "moody"])
        elif emotion.valence < -0.2:
            modifiers.extend(["subdued", "quiet", "contemplative"])

        # Arousal ê¸°ë°˜ ìˆ˜ì •ì
        if emotion.arousal > 0.6:
            modifiers.extend(["dynamic", "energetic", "vibrant", "intense"])
        elif emotion.arousal > 0.2:
            modifiers.extend(["lively", "animated"])
        elif emotion.arousal < -0.6:
            modifiers.extend(["calm", "peaceful", "serene", "tranquil"])
        elif emotion.arousal < -0.2:
            modifiers.extend(["relaxed", "soft"])

        # Dominance ê¸°ë°˜ ìˆ˜ì •ì
        if emotion.dominance > 0.4:
            modifiers.extend(["bold", "confident", "strong"])
        elif emotion.dominance < -0.4:
            modifiers.extend(["delicate", "subtle", "gentle"])

        # ê°ì • ê°•ë„ ê¸°ë°˜ ìˆ˜ì •ì
        intensity = np.sqrt(
            emotion.valence**2 + emotion.arousal**2 + emotion.dominance**2
        ) / np.sqrt(3)
        if intensity > 0.8:
            modifiers.append("highly detailed")
        elif intensity < 0.3:
            modifiers.append("minimalist")

        return ", ".join(modifiers[:6])  # ìµœëŒ€ 6ê°œ ìˆ˜ì •ì


# =============================================================================
# LoRA ê°œì¸í™” ì‹œìŠ¤í…œ
# =============================================================================


class PersonalizedLoRAManager:
    """ê°œì¸í™”ëœ LoRA ì–´ëŒ‘í„° ê´€ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        base_model_path: str = "runwayml/stable-diffusion-v1-5",
        lora_rank: int = 16,
    ):
        self.base_model_path = base_model_path
        self.lora_rank = lora_rank
        self.device = device
        self.user_adapters = {}
        self.adapter_configs = {}

        # LoRA ì €ì¥ ê²½ë¡œ
        self.lora_dir = Path("user_loras")
        self.lora_dir.mkdir(exist_ok=True)

        if not PEFT_AVAILABLE:
            logger.warning("âš ï¸ PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ LoRA ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤")

    def create_user_lora_config(self, user_id: str) -> Optional[LoraConfig]:
        """ì‚¬ìš©ìë³„ LoRA ì„¤ì • ìƒì„±"""
        if not PEFT_AVAILABLE:
            return None

        try:
            lora_config = LoraConfig(
                r=self.lora_rank,
                lora_alpha=32,
                target_modules=[
                    "to_k",
                    "to_q",
                    "to_v",
                    "to_out.0",
                    "proj_in",
                    "proj_out",
                    "ff.net.0.proj",
                    "ff.net.2",
                ],
                lora_dropout=0.1,
                bias="none",
                task_type=TaskType.DIFFUSION,
            )

            self.adapter_configs[user_id] = lora_config
            logger.info(f"âœ… ì‚¬ìš©ì {user_id}ì˜ LoRA ì„¤ì • ìƒì„± ì™„ë£Œ")
            return lora_config

        except Exception as e:
            logger.error(f"âŒ LoRA ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def save_user_lora(self, user_id: str, model_state_dict: Dict[str, torch.Tensor]):
        """ì‚¬ìš©ì LoRA ì–´ëŒ‘í„° ì €ì¥"""
        try:
            user_lora_path = self.lora_dir / f"{user_id}_lora.pt"
            torch.save(model_state_dict, user_lora_path)
            logger.info(f"âœ… ì‚¬ìš©ì {user_id} LoRA ì €ì¥: {user_lora_path}")
        except Exception as e:
            logger.error(f"âŒ LoRA ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_user_lora(self, user_id: str) -> Optional[Dict[str, torch.Tensor]]:
        """ì‚¬ìš©ì LoRA ì–´ëŒ‘í„° ë¡œë“œ"""
        try:
            user_lora_path = self.lora_dir / f"{user_id}_lora.pt"
            if user_lora_path.exists():
                state_dict = torch.load(user_lora_path, map_location=self.device)
                logger.info(f"âœ… ì‚¬ìš©ì {user_id} LoRA ë¡œë“œ: {user_lora_path}")
                return state_dict
        except Exception as e:
            logger.error(f"âŒ LoRA ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

    def get_user_adapter_info(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì–´ëŒ‘í„° ì •ë³´ ë°˜í™˜"""
        return {
            "user_id": user_id,
            "lora_rank": self.lora_rank,
            "config": self.adapter_configs.get(user_id),
            "path": self.lora_dir / f"{user_id}_lora.pt",
        }


# =============================================================================
# ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ê´€ë¦¬
# =============================================================================


class UserEmotionProfile:
    """ì‚¬ìš©ì ê°ì • í”„ë¡œíŒŒì¼ ë° ê°œì¸í™” ë°ì´í„° ê´€ë¦¬"""

    def __init__(self, user_id: str, db_path: str = "user_profiles.db"):
        self.user_id = user_id
        self.db_path = db_path
        self.emotion_history: List[Dict] = []
        self.feedback_history: List[Dict] = []

        # ê°œì¸í™” ì„ í˜¸ë„ ê°€ì¤‘ì¹˜
        self.preference_weights = {
            "color_temperature": 0.0,  # -1.0 (ì°¨ê°€ìš´) to 1.0 (ë”°ëœ»í•œ)
            "brightness": 0.0,  # -1.0 (ì–´ë‘ìš´) to 1.0 (ë°ì€)
            "saturation": 0.0,  # -1.0 (ë¬´ì±„ìƒ‰) to 1.0 (ì±„ë„ ë†’ì€)
            "contrast": 0.0,  # -1.0 (ë‚®ì€ ëŒ€ë¹„) to 1.0 (ë†’ì€ ëŒ€ë¹„)
            "complexity": 0.0,  # -1.0 (ë‹¨ìˆœ) to 1.0 (ë³µì¡)
            "art_style": "realistic",  # realistic, abstract, impressionist, minimalist
            "composition": "balanced",  # minimal, balanced, complex
        }

        # ì¹˜ë£Œ ì§„í–‰ë„ ì§€í‘œ
        self.therapeutic_progress = {
            "mood_trend": 0.0,  # ê°ì • ë³€í™” íŠ¸ë Œë“œ
            "stability_score": 0.0,  # ê°ì • ì•ˆì •ì„±
            "engagement_level": 0.0,  # ì°¸ì—¬ë„
            "recovery_indicator": 0.0,  # íšŒë³µ ì§€í‘œ
        }

        # í•™ìŠµ ë©”íƒ€ë°ì´í„°
        self.learning_metadata = {
            "total_interactions": 0,
            "positive_feedback_rate": 0.0,
            "last_training_date": None,
            "model_version": 1,
        }

        self._init_database()
        self._load_profile()

    def _init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # ê°ì • íˆìŠ¤í† ë¦¬ í…Œì´ë¸”
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS emotion_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                input_text TEXT,
                valence REAL,
                arousal REAL,
                dominance REAL,
                confidence REAL,
                generated_prompt TEXT,
                image_path TEXT
            )
        """
        )

        # í”¼ë“œë°± íˆìŠ¤í† ë¦¬ í…Œì´ë¸”
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS feedback_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                emotion_id INTEGER,
                timestamp TEXT NOT NULL,
                feedback_score REAL,
                feedback_type TEXT,
                comments TEXT,
                FOREIGN KEY (emotion_id) REFERENCES emotion_history (id)
            )
        """
        )

        # ì‚¬ìš©ì í”„ë¡œíŒŒì¼ í…Œì´ë¸”
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS user_profiles (
                user_id TEXT PRIMARY KEY,
                preference_weights TEXT,
                therapeutic_progress TEXT,
                learning_metadata TEXT,
                last_updated TEXT
            )
        """
        )

        conn.commit()
        conn.close()
        logger.info(f"âœ… ì‚¬ìš©ì {self.user_id} ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def _load_profile(self):
        """í”„ë¡œíŒŒì¼ ë°ì´í„° ë¡œë“œ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            # ê°ì • íˆìŠ¤í† ë¦¬ ë¡œë“œ (ìµœê·¼ 50ê°œ)
            cursor.execute(
                """
                SELECT * FROM emotion_history 
                WHERE user_id = ? 
                ORDER BY timestamp DESC 
                LIMIT 50
            """,
                (self.user_id,),
            )

            emotion_rows = cursor.fetchall()
            for row in emotion_rows:
                self.emotion_history.append(
                    {
                        "id": row[0],
                        "timestamp": row[2],
                        "input_text": row[3],
                        "emotion": EmotionEmbedding(row[4], row[5], row[6], row[7]),
                        "generated_prompt": row[8],
                        "image_path": row[9],
                    }
                )

            # í”¼ë“œë°± íˆìŠ¤í† ë¦¬ ë¡œë“œ
            cursor.execute(
                """
                SELECT * FROM feedback_history 
                WHERE user_id = ? 
                ORDER BY timestamp DESC 
                LIMIT 50
            """,
                (self.user_id,),
            )

            feedback_rows = cursor.fetchall()
            for row in feedback_rows:
                self.feedback_history.append(
                    {
                        "id": row[0],
                        "emotion_id": row[2],
                        "timestamp": row[3],
                        "feedback_score": row[4],
                        "feedback_type": row[5],
                        "comments": row[6],
                    }
                )

            # í”„ë¡œíŒŒì¼ ì„¤ì • ë¡œë“œ
            cursor.execute(
                "SELECT * FROM user_profiles WHERE user_id = ?", (self.user_id,)
            )
            profile_row = cursor.fetchone()

            if profile_row:
                self.preference_weights.update(json.loads(profile_row[1]))
                self.therapeutic_progress.update(json.loads(profile_row[2]))
                self.learning_metadata.update(json.loads(profile_row[3]))

            logger.info(
                f"âœ… ì‚¬ìš©ì {self.user_id} í”„ë¡œíŒŒì¼ ë¡œë“œ: ê°ì • {len(self.emotion_history)}ê°œ, í”¼ë“œë°± {len(self.feedback_history)}ê°œ"
            )

        except Exception as e:
            logger.error(f"âŒ í”„ë¡œíŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        finally:
            conn.close()

    def add_emotion_record(
        self,
        input_text: str,
        emotion: EmotionEmbedding,
        generated_prompt: str,
        image_path: str = None,
    ) -> int:
        """ê°ì • ê¸°ë¡ ì¶”ê°€"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            timestamp = datetime.now().isoformat()
            cursor.execute(
                """
                INSERT INTO emotion_history 
                (user_id, timestamp, input_text, valence, arousal, dominance, confidence, generated_prompt, image_path)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    self.user_id,
                    timestamp,
                    input_text,
                    emotion.valence,
                    emotion.arousal,
                    emotion.dominance,
                    emotion.confidence,
                    generated_prompt,
                    image_path,
                ),
            )

            emotion_id = cursor.lastrowid
            conn.commit()

            # ë©”ëª¨ë¦¬ì—ë„ ì¶”ê°€
            self.emotion_history.append(
                {
                    "id": emotion_id,
                    "timestamp": timestamp,
                    "input_text": input_text,
                    "emotion": emotion,
                    "generated_prompt": generated_prompt,
                    "image_path": image_path,
                }
            )

            logger.info(f"âœ… ê°ì • ê¸°ë¡ ì¶”ê°€: ID {emotion_id}")
            return emotion_id

        except Exception as e:
            logger.error(f"âŒ ê°ì • ê¸°ë¡ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return -1
        finally:
            conn.close()

    def add_feedback(
        self,
        emotion_id: int,
        feedback_score: float,
        feedback_type: str = "rating",
        comments: str = None,
    ):
        """í”¼ë“œë°± ì¶”ê°€"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            timestamp = datetime.now().isoformat()
            cursor.execute(
                """
                INSERT INTO feedback_history 
                (user_id, emotion_id, timestamp, feedback_score, feedback_type, comments)
                VALUES (?, ?, ?, ?, ?, ?)
            """,
                (
                    self.user_id,
                    emotion_id,
                    timestamp,
                    feedback_score,
                    feedback_type,
                    comments,
                ),
            )

            conn.commit()

            # ë©”ëª¨ë¦¬ì—ë„ ì¶”ê°€
            self.feedback_history.append(
                {
                    "emotion_id": emotion_id,
                    "timestamp": timestamp,
                    "feedback_score": feedback_score,
                    "feedback_type": feedback_type,
                    "comments": comments,
                }
            )

            # ê°œì¸í™” ì„ í˜¸ë„ ì—…ë°ì´íŠ¸
            self._update_preferences_from_feedback(feedback_score)

            # ì¹˜ë£Œ ì§„í–‰ë„ ì—…ë°ì´íŠ¸
            self._update_therapeutic_progress()

            # í”„ë¡œíŒŒì¼ ì €ì¥
            self._save_profile()

            logger.info(f"âœ… í”¼ë“œë°± ì¶”ê°€: ê°ì • ID {emotion_id}, ì ìˆ˜ {feedback_score}")

        except Exception as e:
            logger.error(f"âŒ í”¼ë“œë°± ì¶”ê°€ ì‹¤íŒ¨: {e}")
        finally:
            conn.close()

    def _update_preferences_from_feedback(self, feedback_score: float):
        """í”¼ë“œë°± ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ í˜¸ë„ ì—…ë°ì´íŠ¸"""
        learning_rate = 0.1

        if feedback_score > 3.0:  # ê¸ì •ì  í”¼ë“œë°± (1-5 ì²™ë„)
            weight = (feedback_score - 3.0) / 2.0 * learning_rate

            # ìµœê·¼ ê°ì • ê¸°ë°˜ ì„ í˜¸ë„ ì¡°ì •
            if self.emotion_history:
                recent_emotion = self.emotion_history[-1]["emotion"]

                # Valence ê¸°ë°˜ ë°ê¸°/ì±„ë„ ì¡°ì •
                if recent_emotion.valence > 0:
                    self.preference_weights["brightness"] += weight * 0.1
                    self.preference_weights["saturation"] += weight * 0.1
                else:
                    self.preference_weights["brightness"] -= weight * 0.05
                    self.preference_weights["saturation"] -= weight * 0.05

                # Arousal ê¸°ë°˜ ëŒ€ë¹„/ë³µì¡ì„± ì¡°ì •
                if recent_emotion.arousal > 0:
                    self.preference_weights["contrast"] += weight * 0.1
                    self.preference_weights["complexity"] += weight * 0.05
                else:
                    self.preference_weights["contrast"] -= weight * 0.05
                    self.preference_weights["complexity"] -= weight * 0.1

        # ë²”ìœ„ ì œí•œ
        for key in self.preference_weights:
            if isinstance(self.preference_weights[key], (int, float)):
                self.preference_weights[key] = np.clip(
                    self.preference_weights[key], -1.0, 1.0
                )

    def _update_therapeutic_progress(self):
        """ì¹˜ë£Œ ì§„í–‰ë„ ì—…ë°ì´íŠ¸"""
        if len(self.emotion_history) < 3:
            return

        # ìµœê·¼ ê°ì •ë“¤ì˜ Valence íŠ¸ë Œë“œ ë¶„ì„
        recent_valences = [
            entry["emotion"].valence for entry in self.emotion_history[-10:]
        ]
        if len(recent_valences) >= 3:
            # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
            x = np.arange(len(recent_valences))
            y = np.array(recent_valences)
            slope = np.corrcoef(x, y)[0, 1] if len(recent_valences) > 1 else 0
            self.therapeutic_progress["mood_trend"] = slope

        # ê°ì • ì•ˆì •ì„± (ë³€ë™ì„±ì˜ ì—­ìˆ˜)
        if len(recent_valences) >= 5:
            stability = 1.0 / (1.0 + np.std(recent_valences))
            self.therapeutic_progress["stability_score"] = stability

        # ì°¸ì—¬ë„ (í”¼ë“œë°± ì œê³µë¥ )
        if self.feedback_history:
            recent_interactions = len(self.emotion_history[-20:])
            recent_feedbacks = len(
                [
                    f
                    for f in self.feedback_history[-20:]
                    if f["emotion_id"] in [e["id"] for e in self.emotion_history[-20:]]
                ]
            )
            engagement = recent_feedbacks / max(1, recent_interactions)
            self.therapeutic_progress["engagement_level"] = engagement

        # íšŒë³µ ì§€í‘œ (ê¸ì •ì  í”¼ë“œë°± ë¹„ìœ¨ + ê°ì • íŠ¸ë Œë“œ)
        if self.feedback_history:
            positive_feedbacks = len(
                [f for f in self.feedback_history[-20:] if f["feedback_score"] > 3.0]
            )
            total_feedbacks = len(self.feedback_history[-20:])
            positive_rate = positive_feedbacks / max(1, total_feedbacks)

            recovery = (
                positive_rate
                + max(0, self.therapeutic_progress["mood_trend"])
                + self.therapeutic_progress["stability_score"]
            ) / 3.0
            self.therapeutic_progress["recovery_indicator"] = recovery

    def _save_profile(self):
        """í”„ë¡œíŒŒì¼ ë°ì´í„° ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            timestamp = datetime.now().isoformat()
            cursor.execute(
                """
                INSERT OR REPLACE INTO user_profiles 
                (user_id, preference_weights, therapeutic_progress, learning_metadata, last_updated)
                VALUES (?, ?, ?, ?, ?)
            """,
                (
                    self.user_id,
                    json.dumps(self.preference_weights),
                    json.dumps(self.therapeutic_progress),
                    json.dumps(self.learning_metadata),
                    timestamp,
                ),
            )

            conn.commit()
            logger.info(f"âœ… ì‚¬ìš©ì {self.user_id} í”„ë¡œíŒŒì¼ ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ í”„ë¡œíŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        finally:
            conn.close()

    def get_personalized_style_modifiers(self) -> str:
        """ê°œì¸í™”ëœ ìŠ¤íƒ€ì¼ ìˆ˜ì •ì ìƒì„±"""
        modifiers = []

        # ìƒ‰ì˜¨ë„ ê¸°ë°˜
        if self.preference_weights["color_temperature"] > 0.3:
            modifiers.append("warm lighting")
        elif self.preference_weights["color_temperature"] < -0.3:
            modifiers.append("cool lighting")

        # ë°ê¸° ê¸°ë°˜
        if self.preference_weights["brightness"] > 0.3:
            modifiers.append("bright")
        elif self.preference_weights["brightness"] < -0.3:
            modifiers.append("dim lighting")

        # ì±„ë„ ê¸°ë°˜
        if self.preference_weights["saturation"] > 0.3:
            modifiers.append("vibrant colors")
        elif self.preference_weights["saturation"] < -0.3:
            modifiers.append("muted colors")

        # ëŒ€ë¹„ ê¸°ë°˜
        if self.preference_weights["contrast"] > 0.3:
            modifiers.append("high contrast")
        elif self.preference_weights["contrast"] < -0.3:
            modifiers.append("soft contrast")

        # ë³µì¡ì„± ê¸°ë°˜
        if self.preference_weights["complexity"] > 0.3:
            modifiers.append("detailed")
        elif self.preference_weights["complexity"] < -0.3:
            modifiers.append("minimalist")

        # ì•„íŠ¸ ìŠ¤íƒ€ì¼
        modifiers.append(f"{self.preference_weights['art_style']} style")

        return ", ".join(modifiers)

    def get_therapeutic_insights(self) -> Dict[str, Any]:
        """ì¹˜ë£Œì  ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        if len(self.emotion_history) < 3:
            return {
                "message": "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "status": "insufficient_data",
            }

        insights = {
            "emotional_state": {
                "current_mood": self._get_current_mood_description(),
                "mood_trend": self.therapeutic_progress["mood_trend"],
                "stability": self.therapeutic_progress["stability_score"],
            },
            "progress_indicators": {
                "engagement_level": self.therapeutic_progress["engagement_level"],
                "recovery_indicator": self.therapeutic_progress["recovery_indicator"],
                "total_interactions": len(self.emotion_history),
                "feedback_count": len(self.feedback_history),
            },
            "recommendations": self._generate_recommendations(),
            "preference_summary": self.preference_weights,
        }

        return insights

    def _get_current_mood_description(self) -> str:
        """í˜„ì¬ ê¸°ë¶„ ìƒíƒœ ì„¤ëª…"""
        if not self.emotion_history:
            return "ë°ì´í„° ì—†ìŒ"

        recent_emotions = [entry["emotion"] for entry in self.emotion_history[-5:]]
        avg_valence = np.mean([e.valence for e in recent_emotions])
        avg_arousal = np.mean([e.arousal for e in recent_emotions])

        if avg_valence > 0.3 and avg_arousal > 0.3:
            return "í™œê¸°ì°¬ ê¸ì • ìƒíƒœ"
        elif avg_valence > 0.3 and avg_arousal < -0.3:
            return "í‰ì˜¨í•œ ê¸ì • ìƒíƒœ"
        elif avg_valence < -0.3 and avg_arousal > 0.3:
            return "ë¶ˆì•ˆí•œ ë¶€ì • ìƒíƒœ"
        elif avg_valence < -0.3 and avg_arousal < -0.3:
            return "ìš°ìš¸í•œ ìƒíƒœ"
        else:
            return "ì¤‘ì„±ì  ìƒíƒœ"

    def _generate_recommendations(self) -> List[str]:
        """ê°œì¸í™”ëœ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ê°ì • íŠ¸ë Œë“œ ê¸°ë°˜
        if self.therapeutic_progress["mood_trend"] < -0.3:
            recommendations.append(
                "ë¶€ì •ì ì¸ ê°ì • íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸ì •ì ì¸ í™œë™ì´ë‚˜ ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œë„í•´ë³´ì„¸ìš”."
            )
        elif self.therapeutic_progress["mood_trend"] > 0.3:
            recommendations.append(
                "ê°ì • ìƒíƒœê°€ ê°œì„ ë˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ íŒ¨í„´ì„ ìœ ì§€í•˜ì„¸ìš”."
            )

        # ì•ˆì •ì„± ê¸°ë°˜
        if self.therapeutic_progress["stability_score"] < 0.5:
            recommendations.append(
                "ê°ì • ë³€ë™ì´ í½ë‹ˆë‹¤. ê·œì¹™ì ì¸ ì‚¬ìš©ê³¼ ì¼ê´€ëœ í”¼ë“œë°±ì´ ë„ì›€ë  ê²ƒì…ë‹ˆë‹¤."
            )

        # ì°¸ì—¬ë„ ê¸°ë°˜
        if self.therapeutic_progress["engagement_level"] < 0.3:
            recommendations.append(
                "ë” ìì£¼ í”¼ë“œë°±ì„ ì œê³µí•˜ì‹œë©´ ê°œì¸í™” íš¨ê³¼ê°€ í–¥ìƒë©ë‹ˆë‹¤."
            )

        # ê¸°ë³¸ ì¶”ì²œ
        if not recommendations:
            recommendations.append(
                "í˜„ì¬ ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆë‹¤. ì§€ì†ì ì¸ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
            )

        return recommendations


# =============================================================================
# DRaFT+ ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ
# =============================================================================


class DRaFTPlusRewardModel:
    """DRaFT+ ë°©ì‹ì˜ ê°œì„ ëœ ë³´ìƒ ëª¨ë¸"""

    def __init__(self, device: torch.device = None):
        self.device = device if device else torch.device("cpu")

        # ê°ì • ì •í™•ë„ í‰ê°€ê¸°
        self.emotion_evaluator = self._build_emotion_evaluator()

        # ë¯¸ì  í’ˆì§ˆ í‰ê°€ê¸°
        self.aesthetic_evaluator = self._build_aesthetic_evaluator()

        # ê°œì¸í™” ì ìˆ˜ í‰ê°€ê¸°
        self.personalization_evaluator = self._build_personalization_evaluator()

        # ë‹¤ì–‘ì„± í‰ê°€ê¸°
        self.diversity_evaluator = self._build_diversity_evaluator()

        logger.info("âœ… DRaFT+ ë³´ìƒ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _build_emotion_evaluator(self) -> nn.Module:
        """ê°ì • ì •í™•ë„ í‰ê°€ê¸°"""
        return nn.Sequential(
            nn.Linear(768, 512),  # CLIP ì„ë² ë”© í¬ê¸° ê°€ì •
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 3),  # VAD ì˜ˆì¸¡
            nn.Tanh(),
        ).to(self.device)

    def _build_aesthetic_evaluator(self) -> nn.Module:
        """ë¯¸ì  í’ˆì§ˆ í‰ê°€ê¸°"""
        return nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((4, 4)),
            nn.Flatten(),
            nn.Linear(128 * 4 * 4, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        ).to(self.device)

    def _build_personalization_evaluator(self) -> nn.Module:
        """ê°œì¸í™” ì ìˆ˜ í‰ê°€ê¸°"""
        return nn.Sequential(
            nn.Linear(512 + 7, 256),  # ì´ë¯¸ì§€ íŠ¹ì„± + ê°œì¸í™” ì„ í˜¸ë„
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid(),
        ).to(self.device)

    def _build_diversity_evaluator(self) -> nn.Module:
        """ë‹¤ì–‘ì„± í‰ê°€ê¸°"""
        return nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid(),
        ).to(self.device)

    def calculate_comprehensive_reward(
        self,
        generated_images: torch.Tensor,
        target_emotion: EmotionEmbedding,
        user_profile: UserEmotionProfile,
        previous_images: List[torch.Tensor] = None,
    ) -> torch.Tensor:
        """ì¢…í•©ì ì¸ ë³´ìƒ ê³„ì‚° (DRaFT+ ë°©ì‹)"""
        batch_size = generated_images.shape[0]

        try:
            with torch.no_grad():
                # 1. ê°ì • ì •í™•ë„ ë³´ìƒ
                emotion_reward = self._calculate_emotion_reward(
                    generated_images, target_emotion
                )

                # 2. ë¯¸ì  í’ˆì§ˆ ë³´ìƒ
                aesthetic_reward = self._calculate_aesthetic_reward(generated_images)

                # 3. ê°œì¸í™” ë³´ìƒ
                personalization_reward = self._calculate_personalization_reward(
                    generated_images, user_profile
                )

                # 4. ë‹¤ì–‘ì„± ë³´ìƒ (DRaFT+ ì¶”ê°€ ìš”ì†Œ)
                diversity_reward = self._calculate_diversity_reward(
                    generated_images, previous_images
                )

                # 5. ê°€ì¤‘ í•©ê³„ (DRaFT+ ê°€ì¤‘ì¹˜)
                total_reward = (
                    0.35 * emotion_reward
                    + 0.25 * aesthetic_reward
                    + 0.25 * personalization_reward
                    + 0.15 * diversity_reward
                )

                # 6. ì •ê·œí™” ë° ìŠ¤ë¬´ë”©
                total_reward = torch.clamp(total_reward, 0.0, 1.0)

            return total_reward

        except Exception as e:
            logger.warning(f"âš ï¸ ë³´ìƒ ê³„ì‚° ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ë°˜í™˜")
            return torch.tensor([0.5] * batch_size, device=self.device)

    def _calculate_emotion_reward(
        self, images: torch.Tensor, target_emotion: EmotionEmbedding
    ) -> torch.Tensor:
        """ê°ì • ì •í™•ë„ ê¸°ë°˜ ë³´ìƒ"""
        batch_size = images.shape[0]

        # ê°„ë‹¨í•œ ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” CLIP ë“± ì‚¬ìš©)
        image_features = self._extract_simple_features(images)

        # ëª©í‘œ ê°ì •ê³¼ì˜ ì¼ì¹˜ë„ ê³„ì‚°
        target_vector = torch.tensor(
            [target_emotion.valence, target_emotion.arousal, target_emotion.dominance],
            device=self.device,
        ).repeat(batch_size, 1)

        predicted_emotions = self.emotion_evaluator(image_features)
        emotion_distance = F.mse_loss(
            predicted_emotions, target_vector, reduction="none"
        ).mean(dim=1)

        # ê±°ë¦¬ë¥¼ ë³´ìƒìœ¼ë¡œ ë³€í™˜ (ê±°ë¦¬ê°€ í´ìˆ˜ë¡ ë³´ìƒ ë‚®ìŒ)
        emotion_reward = torch.exp(-emotion_distance * 2.0)

        return emotion_reward

    def _calculate_aesthetic_reward(self, images: torch.Tensor) -> torch.Tensor:
        """ë¯¸ì  í’ˆì§ˆ ë³´ìƒ"""
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (í•„ìš”ì‹œ)
        if images.shape[-1] != 64:  # ì˜ˆì‹œ í¬ê¸°
            images_resized = F.interpolate(
                images, size=(64, 64), mode="bilinear", align_corners=False
            )
        else:
            images_resized = images

        aesthetic_scores = self.aesthetic_evaluator(images_resized).squeeze()

        # ë°°ì¹˜ ì°¨ì› ë³´ì¥
        if aesthetic_scores.dim() == 0:
            aesthetic_scores = aesthetic_scores.unsqueeze(0)

        return aesthetic_scores

    def _calculate_personalization_reward(
        self, images: torch.Tensor, user_profile: UserEmotionProfile
    ) -> torch.Tensor:
        """ê°œì¸í™” ë³´ìƒ"""
        batch_size = images.shape[0]

        # ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ
        image_features = self._extract_simple_features(images)

        # ì‚¬ìš©ì ì„ í˜¸ë„ ë²¡í„° ìƒì„±
        preference_vector = torch.tensor(
            [
                user_profile.preference_weights["color_temperature"],
                user_profile.preference_weights["brightness"],
                user_profile.preference_weights["saturation"],
                user_profile.preference_weights["contrast"],
                user_profile.preference_weights["complexity"],
                (
                    1.0
                    if user_profile.preference_weights["art_style"] == "realistic"
                    else 0.0
                ),
                (
                    1.0
                    if user_profile.preference_weights["composition"] == "balanced"
                    else 0.0
                ),
            ],
            device=self.device,
        ).repeat(batch_size, 1)

        # ê°œì¸í™” íŠ¹ì„±ê³¼ ê²°í•©
        combined_features = torch.cat([image_features, preference_vector], dim=1)
        personalization_scores = self.personalization_evaluator(
            combined_features
        ).squeeze()

        if personalization_scores.dim() == 0:
            personalization_scores = personalization_scores.unsqueeze(0)

        return personalization_scores

    def _calculate_diversity_reward(
        self, images: torch.Tensor, previous_images: List[torch.Tensor] = None
    ) -> torch.Tensor:
        """ë‹¤ì–‘ì„± ë³´ìƒ (DRaFT+ í•µì‹¬ ìš”ì†Œ)"""
        batch_size = images.shape[0]

        if previous_images is None or len(previous_images) == 0:
            # ì´ì „ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ìµœëŒ€ ë‹¤ì–‘ì„± ë³´ìƒ
            return torch.ones(batch_size, device=self.device)

        # í˜„ì¬ ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ
        current_features = self._extract_simple_features(images)

        # ì´ì „ ì´ë¯¸ì§€ë“¤ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°
        diversity_scores = []

        for img_features in current_features:
            min_distance = float("inf")

            for prev_img in previous_images[-5:]:  # ìµœê·¼ 5ê°œ ì´ë¯¸ì§€ì™€ ë¹„êµ
                if prev_img.shape[0] == 1:  # ë°°ì¹˜ í¬ê¸° 1ì¸ ê²½ìš°
                    prev_features = self._extract_simple_features(prev_img).squeeze(0)
                    distance = F.pairwise_distance(
                        img_features.unsqueeze(0), prev_features.unsqueeze(0)
                    )
                    min_distance = min(min_distance, distance.item())

            # ê±°ë¦¬ ê¸°ë°˜ ë‹¤ì–‘ì„± ì ìˆ˜ (ê±°ë¦¬ê°€ í´ìˆ˜ë¡ ë‹¤ì–‘ì„± ë†’ìŒ)
            diversity_score = min(1.0, min_distance / 10.0)  # ì •ê·œí™”
            diversity_scores.append(diversity_score)

        return torch.tensor(diversity_scores, device=self.device)

    def _extract_simple_features(self, images: torch.Tensor) -> torch.Tensor:
        """ê°„ë‹¨í•œ ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ"""
        batch_size = images.shape[0]

        # ê¸°ë³¸ì ì¸ í†µê³„ì  íŠ¹ì„±ë“¤
        features = []

        for i in range(batch_size):
            img = images[i]

            # ìƒ‰ìƒ í†µê³„
            mean_rgb = img.mean(dim=[1, 2])  # RGB í‰ê· 
            std_rgb = img.std(dim=[1, 2])  # RGB í‘œì¤€í¸ì°¨

            # ë°ê¸° ë° ëŒ€ë¹„
            gray = 0.299 * img[0] + 0.587 * img[1] + 0.114 * img[2]
            brightness = gray.mean()
            contrast = gray.std()

            # ì—ì§€ ë°€ë„ (ê°„ë‹¨í•œ ê·¼ì‚¬)
            grad_x = torch.abs(gray[1:, :] - gray[:-1, :]).mean()
            grad_y = torch.abs(gray[:, 1:] - gray[:, :-1]).mean()
            edge_density = (grad_x + grad_y) / 2

            # íŠ¹ì„± ë²¡í„° êµ¬ì„±
            feature_vector = torch.cat(
                [
                    mean_rgb,
                    std_rgb,
                    brightness.unsqueeze(0),
                    contrast.unsqueeze(0),
                    edge_density.unsqueeze(0),
                ]
            )

            # 512ì°¨ì›ìœ¼ë¡œ íŒ¨ë”© (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŠ¹ì„± ì¶”ì¶œ í•„ìš”)
            if feature_vector.shape[0] < 512:
                padding = torch.zeros(512 - feature_vector.shape[0], device=self.device)
                feature_vector = torch.cat([feature_vector, padding])

            features.append(feature_vector[:512])

        return torch.stack(features)


class DRaFTPlusTrainer:
    """DRaFT+ ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë„ˆ"""

    def __init__(
        self, pipeline, reward_model: DRaFTPlusRewardModel, learning_rate: float = 1e-5
    ):
        self.pipeline = pipeline
        self.reward_model = reward_model
        self.device = device
        self.learning_rate = learning_rate

        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        if hasattr(pipeline, "unet") and hasattr(pipeline.unet, "parameters"):
            self.optimizer = optim.AdamW(
                pipeline.unet.parameters(),
                lr=learning_rate,
                weight_decay=0.01,
                eps=1e-8,
            )
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=100, eta_min=learning_rate * 0.1
            )
            self.can_train = True
            logger.info("âœ… DRaFT+ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ UNetì´ ì—†ì–´ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‹¤í–‰")
            self.can_train = False

        # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ì´ë¯¸ì§€ íˆìŠ¤í† ë¦¬
        self.image_history = []
        self.max_history_size = 10

    def train_step(
        self,
        prompt: str,
        target_emotion: EmotionEmbedding,
        user_profile: UserEmotionProfile,
        num_inference_steps: int = 8,
        diversity_weight: float = 0.15,
    ) -> Dict[str, float]:
        """DRaFT+ í•™ìŠµ ìŠ¤í…"""

        if not self.can_train:
            # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
            return {
                "emotion_reward": random.uniform(0.4, 0.8),
                "aesthetic_reward": random.uniform(0.5, 0.9),
                "personalization_reward": random.uniform(0.3, 0.7),
                "diversity_reward": random.uniform(0.6, 1.0),
                "total_reward": random.uniform(0.5, 0.8),
                "loss": random.uniform(0.2, 0.6),
                "learning_rate": self.learning_rate,
                "mode": "simulation",
            }

        try:
            # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
            self.optimizer.zero_grad()

            # UNet í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
            self.pipeline.unet.train()

            # ì´ë¯¸ì§€ ìƒì„± (ê°„ì†Œí™”ëœ ë””í“¨ì „ ê³¼ì •)
            with torch.enable_grad():
                # í…ìŠ¤íŠ¸ ì„ë² ë”©
                text_embeddings = self._encode_prompt(prompt)

                # ë…¸ì´ì¦ˆ ìƒì„±
                latents = torch.randn(
                    (1, 4, 64, 64),  # SD 1.5 ê¸°ë³¸ latent í¬ê¸°
                    device=self.device,
                    dtype=text_embeddings.dtype,
                    requires_grad=True,
                )

                # ê°„ì†Œí™”ëœ ë””ë…¸ì´ì§• (ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´)
                for step in range(num_inference_steps):
                    t = torch.tensor(
                        [1000 - step * (1000 // num_inference_steps)],
                        device=self.device,
                    )

                    # UNet ì˜ˆì¸¡
                    noise_pred = self.pipeline.unet(
                        latents,
                        t,
                        encoder_hidden_states=text_embeddings,
                        return_dict=False,
                    )[0]

                    # ë””ë…¸ì´ì§• ìŠ¤í…
                    latents = latents - 0.1 * noise_pred

                # VAE ë””ì½”ë”© (ê°€ëŠ¥í•œ ê²½ìš°)
                if hasattr(self.pipeline, "vae"):
                    try:
                        if hasattr(self.pipeline.vae.config, "scaling_factor"):
                            latents_scaled = (
                                latents / self.pipeline.vae.config.scaling_factor
                            )
                        else:
                            latents_scaled = latents

                        images = self.pipeline.vae.decode(
                            latents_scaled, return_dict=False
                        )[0]
                        images = (images / 2 + 0.5).clamp(0, 1)
                    except:
                        # VAE ë””ì½”ë”© ì‹¤íŒ¨ì‹œ ê°€ì§œ ì´ë¯¸ì§€
                        images = torch.rand(1, 3, 512, 512, device=self.device)
                else:
                    images = torch.rand(1, 3, 512, 512, device=self.device)

                # ë³´ìƒ ê³„ì‚°
                rewards = self.reward_model.calculate_comprehensive_reward(
                    images, target_emotion, user_profile, self.image_history
                )

                # DRaFT+ ì†ì‹¤ ê³„ì‚° (ë‹¤ì–‘ì„± ì •ê·œí™” í¬í•¨)
                reward_loss = -rewards.mean()

                # ë‹¤ì–‘ì„± ì •ê·œí™” ì†ì‹¤
                diversity_loss = self._calculate_diversity_loss(images)

                # ì´ ì†ì‹¤
                total_loss = reward_loss + diversity_weight * diversity_loss

                # ì—­ì „íŒŒ
                total_loss.backward()

                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(self.pipeline.unet.parameters(), 1.0)

                # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                self.optimizer.step()
                self.scheduler.step()

                # ì´ë¯¸ì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                self._update_image_history(images.detach())

                # ê²°ê³¼ ë°˜í™˜
                with torch.no_grad():
                    return {
                        "total_reward": rewards.mean().item(),
                        "reward_loss": reward_loss.item(),
                        "diversity_loss": diversity_loss.item(),
                        "total_loss": total_loss.item(),
                        "learning_rate": self.scheduler.get_last_lr()[0],
                        "mode": "training",
                    }

        except Exception as e:
            logger.error(f"âŒ DRaFT+ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {
                "error": str(e),
                "total_reward": 0.5,
                "loss": 1.0,
                "learning_rate": self.learning_rate,
                "mode": "error",
            }

    def _encode_prompt(self, prompt: str) -> torch.Tensor:
        """í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©"""
        try:
            text_inputs = self.pipeline.tokenizer(
                prompt,
                padding="max_length",
                max_length=self.pipeline.tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt",
            ).to(self.device)

            text_embeddings = self.pipeline.text_encoder(text_inputs.input_ids)[0]
            return text_embeddings

        except Exception as e:
            logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”© ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return torch.randn(1, 77, 768, device=self.device)

    def _calculate_diversity_loss(self, images: torch.Tensor) -> torch.Tensor:
        """ë‹¤ì–‘ì„± ì†ì‹¤ ê³„ì‚° (DRaFT+ í•µì‹¬)"""
        if len(self.image_history) == 0:
            return torch.tensor(0.0, device=self.device)

        # í˜„ì¬ ì´ë¯¸ì§€ì™€ íˆìŠ¤í† ë¦¬ ì´ë¯¸ì§€ë“¤ ê°„ì˜ ìœ ì‚¬ì„± ê³„ì‚°
        current_features = self.reward_model._extract_simple_features(images)

        total_similarity = 0.0
        count = 0

        for hist_img in self.image_history[-3:]:  # ìµœê·¼ 3ê°œì™€ ë¹„êµ
            hist_features = self.reward_model._extract_simple_features(hist_img)
            similarity = F.cosine_similarity(
                current_features, hist_features, dim=1
            ).mean()
            total_similarity += similarity
            count += 1

        if count > 0:
            avg_similarity = total_similarity / count
            # ìœ ì‚¬ì„±ì´ ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì†ì‹¤ ì¦ê°€
            diversity_loss = torch.clamp(avg_similarity, 0.0, 1.0)
        else:
            diversity_loss = torch.tensor(0.0, device=self.device)

        return diversity_loss

    def _update_image_history(self, images: torch.Tensor):
        """ì´ë¯¸ì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        self.image_history.append(images.clone())

        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(self.image_history) > self.max_history_size:
            self.image_history.pop(0)


# =============================================================================
# ë©”ì¸ ì‹œìŠ¤í…œ í†µí•©
# =============================================================================


class EmotionalImageTherapySystem:
    """ê°ì • ê¸°ë°˜ ì´ë¯¸ì§€ ì¹˜ë£Œ ì‹œìŠ¤í…œ"""

    def __init__(self, model_path: str = "runwayml/stable-diffusion-v1-5"):
        self.model_path = model_path
        self.device = device

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = Path("generated_images")
        self.output_dir.mkdir(exist_ok=True)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        logger.info("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")

        # 1. ê°ì • ë§¤í¼ ì´ˆê¸°í™”
        self.emotion_mapper = AdvancedEmotionMapper()

        # 2. LoRA ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.lora_manager = PersonalizedLoRAManager(model_path)

        # 3. SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        self.pipeline = self._load_pipeline()

        # 4. ë³´ìƒ ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        if self.pipeline:
            self.reward_model = DRaFTPlusRewardModel(self.device)
            self.trainer = DRaFTPlusTrainer(self.pipeline, self.reward_model)
        else:
            self.reward_model = None
            self.trainer = None

        # 5. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìºì‹œ
        self.user_profiles = {}

        logger.info("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")

    def _load_pipeline(self):
        """SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if not DIFFUSERS_AVAILABLE:
            logger.error("âŒ Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None

        try:
            logger.info(f"ğŸ“¦ Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘: {self.model_path}")

            pipeline = StableDiffusionPipeline.from_pretrained(
                self.model_path,
                torch_dtype=(
                    # ìœ íš¨í•˜ì§€ ì•Šì€ ìˆ«ìê°€ ë“¤ì–´ê°€ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë¯€ë¡œ ëª¨ë‘ float32ë¡œ ì„¤ì •
                    torch.float32
                    if self.device.type == "mps"
                    else torch.float32
                ),
                use_safetensors=True,
                safety_checker=None,  # ë¹ ë¥¸ ìƒì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
                requires_safety_checker=False,
            )

            # ìµœì í™” ì„¤ì •
            pipeline = pipeline.to(self.device)

            # ë©”ëª¨ë¦¬ ìµœì í™”
            pipeline.enable_attention_slicing()

            if self.device.type == "cuda":
                pipeline.enable_sequential_cpu_offload()

            # ë¹ ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ë³€ê²½
            pipeline.scheduler = EulerDiscreteScheduler.from_config(
                pipeline.scheduler.config
            )

            logger.info("âœ… SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ ë° ìµœì í™” ì™„ë£Œ")
            return pipeline

        except Exception as e:
            logger.error(f"âŒ SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_user_profile(self, user_id: str) -> UserEmotionProfile:
        """ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserEmotionProfile(user_id)
            logger.info(f"âœ… ìƒˆ ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìƒì„±: {user_id}")
        return self.user_profiles[user_id]

    def generate_therapeutic_image(
        self,
        user_id: str,
        input_text: str,
        base_prompt: str = "",
        num_inference_steps: int = 15,
        guidance_scale: float = 7.5,
        width: int = 512,
        height: int = 512,
    ) -> Dict[str, Any]:
        """ì¹˜ë£Œìš© ì´ë¯¸ì§€ ìƒì„±"""

        try:
            logger.info(f"ğŸ¨ ì‚¬ìš©ì {user_id}ì˜ ì´ë¯¸ì§€ ìƒì„± ì‹œì‘")
            logger.info(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {input_text}")

            # 1. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ë¡œë“œ
            user_profile = self.get_user_profile(user_id)

            # 2. ê°ì • ë¶„ì„
            emotion = self.emotion_mapper.extract_emotion_from_text(input_text)
            logger.info(
                f"ğŸ˜Š ê°ì • ë¶„ì„: V={emotion.valence:.3f}, A={emotion.arousal:.3f}, D={emotion.dominance:.3f}"
            )

            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            emotion_modifiers = self.emotion_mapper.emotion_to_prompt_modifiers(emotion)
            personal_modifiers = user_profile.get_personalized_style_modifiers()

            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not base_prompt:
                base_prompt = "digital art, beautiful scene"

            final_prompt = f"{base_prompt}, {emotion_modifiers}, {personal_modifiers}"
            final_prompt += ", high quality, detailed, masterpiece"

            logger.info(f"ğŸ¯ ìµœì¢… í”„ë¡¬í”„íŠ¸: {final_prompt}")

            # 4. ì´ë¯¸ì§€ ìƒì„±
            if self.pipeline:
                # SD íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
                with torch.autocast(
                    self.device.type if self.device.type != "mps" else "cpu"
                ):
                    result = self.pipeline(
                        prompt=final_prompt,
                        num_inference_steps=num_inference_steps,
                        guidance_scale=guidance_scale,
                        width=width,
                        height=height,
                        output_type="pil",
                    )

                generated_image = result.images[0]
                logger.info("âœ… SD íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
            else:
                # í´ë°±: ê°„ë‹¨í•œ ì´ë¯¸ì§€ ìƒì„±
                generated_image = self._generate_fallback_image(emotion, width, height)
                logger.info("âš ï¸ í´ë°± ì´ë¯¸ì§€ ìƒì„±ê¸° ì‚¬ìš©")

            # 5. ì´ë¯¸ì§€ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            image_filename = f"{user_id}_{timestamp}.png"
            image_path = self.output_dir / image_filename
            generated_image.save(image_path)

            # 6. ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ë¡
            emotion_id = user_profile.add_emotion_record(
                input_text=input_text,
                emotion=emotion,
                generated_prompt=final_prompt,
                image_path=str(image_path),
            )

            # 7. ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "emotion_id": emotion_id,
                "user_id": user_id,
                "input_text": input_text,
                "emotion": emotion.to_dict(),
                "final_prompt": final_prompt,
                "image_path": str(image_path),
                "image_filename": image_filename,
                "generation_params": {
                    "num_inference_steps": num_inference_steps,
                    "guidance_scale": guidance_scale,
                    "width": width,
                    "height": height,
                },
                "timestamp": timestamp,
                "device": str(self.device),
            }

            logger.info(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {image_path}")
            return {"success": True, "image": generated_image, "metadata": metadata}

        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metadata": {"user_id": user_id, "input_text": input_text},
            }

    def _generate_fallback_image(
        self, emotion: EmotionEmbedding, width: int = 512, height: int = 512
    ) -> Image.Image:
        """í´ë°± ì´ë¯¸ì§€ ìƒì„± (SD íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ì‹œ)"""

        # ê°ì • ê¸°ë°˜ ìƒ‰ìƒ ìƒì„±
        if emotion.valence > 0.3:
            # ê¸ì •ì  ê°ì • - ë”°ëœ»í•œ ìƒ‰ìƒ
            base_color = [0.9, 0.8, 0.6]  # ë”°ëœ»í•œ ë…¸ë€ìƒ‰
        elif emotion.valence < -0.3:
            # ë¶€ì •ì  ê°ì • - ì°¨ê°€ìš´ ìƒ‰ìƒ
            base_color = [0.6, 0.7, 0.9]  # ì°¨ê°€ìš´ íŒŒë€ìƒ‰
        else:
            # ì¤‘ì„± ê°ì • - ì¤‘ê°„ ìƒ‰ìƒ
            base_color = [0.7, 0.7, 0.8]  # íšŒìƒ‰ë¹›

        # ê°ì„±ë„ ê¸°ë°˜ ê°•ë„ ì¡°ì •
        intensity = 0.5 + abs(emotion.arousal) * 0.5
        base_color = [c * intensity for c in base_color]

        # ê·¸ë¼ë°ì´ì…˜ ì´ë¯¸ì§€ ìƒì„±
        image_array = np.zeros((height, width, 3))

        for i in range(height):
            for j in range(width):
                # ì¤‘ì‹¬ì—ì„œì˜ ê±°ë¦¬ ê¸°ë°˜ ê·¸ë¼ë°ì´ì…˜
                center_x, center_y = width // 2, height // 2
                distance = np.sqrt((j - center_x) ** 2 + (i - center_y) ** 2)
                max_distance = np.sqrt(center_x**2 + center_y**2)

                # ê°ì • ê¸°ë°˜ ê·¸ë¼ë°ì´ì…˜ íŒ¨í„´
                if emotion.dominance > 0:
                    # ì§€ë°°ì  ê°ì • - ì¤‘ì‹¬ì—ì„œ ë°”ê¹¥ìœ¼ë¡œ
                    factor = 1.0 - (distance / max_distance) * 0.5
                else:
                    # ìˆ˜ë™ì  ê°ì • - ë°”ê¹¥ì—ì„œ ì¤‘ì‹¬ìœ¼ë¡œ
                    factor = 0.5 + (distance / max_distance) * 0.5

                image_array[i, j] = [c * factor for c in base_color]

        # numpy ë°°ì—´ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        image_array = np.clip(image_array * 255, 0, 255).astype(np.uint8)
        return Image.fromarray(image_array)

    def process_feedback(
        self,
        user_id: str,
        emotion_id: int,
        feedback_score: float,
        feedback_type: str = "rating",
        comments: str = None,
        enable_training: bool = True,
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”¼ë“œë°± ì²˜ë¦¬ ë° ê°œì¸í™” í•™ìŠµ"""

        try:
            logger.info(f"ğŸ“ ì‚¬ìš©ì {user_id} í”¼ë“œë°± ì²˜ë¦¬: ì ìˆ˜ {feedback_score}")

            # 1. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ë¡œë“œ
            user_profile = self.get_user_profile(user_id)

            # 2. í”¼ë“œë°± ì €ì¥
            user_profile.add_feedback(
                emotion_id=emotion_id,
                feedback_score=feedback_score,
                feedback_type=feedback_type,
                comments=comments,
            )

            # 3. ê°•í™”í•™ìŠµ ìˆ˜í–‰ (ì˜µì…˜)
            training_result = None
            if (
                enable_training and self.trainer and feedback_score != 3.0
            ):  # ì¤‘ì„± í”¼ë“œë°± ì œì™¸

                # í•´ë‹¹ ê°ì • ê¸°ë¡ ì°¾ê¸°
                emotion_record = None
                for record in user_profile.emotion_history:
                    if record.get("id") == emotion_id:
                        emotion_record = record
                        break

                if emotion_record:
                    logger.info("ğŸ¤– ê°œì¸í™” í•™ìŠµ ì‹œì‘...")
                    training_result = self.trainer.train_step(
                        prompt=emotion_record["generated_prompt"],
                        target_emotion=emotion_record["emotion"],
                        user_profile=user_profile,
                        num_inference_steps=8,  # ë¹ ë¥¸ í•™ìŠµ
                    )
                    logger.info(
                        f"âœ… í•™ìŠµ ì™„ë£Œ: ë³´ìƒ {training_result.get('total_reward', 0):.3f}"
                    )

            # 4. LoRA ì–´ëŒ‘í„° ì €ì¥ (ì£¼ê¸°ì )
            if len(user_profile.feedback_history) % 5 == 0:  # 5ë²ˆì§¸ í”¼ë“œë°±ë§ˆë‹¤ ì €ì¥
                self._save_user_lora_if_needed(user_id, user_profile)

            # 5. ì¹˜ë£Œ ì¸ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸
            insights = user_profile.get_therapeutic_insights()

            result = {
                "success": True,
                "feedback_recorded": True,
                "training_performed": training_result is not None,
                "training_result": training_result,
                "therapeutic_insights": insights,
                "total_interactions": len(user_profile.emotion_history),
                "total_feedbacks": len(user_profile.feedback_history),
            }

            logger.info("âœ… í”¼ë“œë°± ì²˜ë¦¬ ì™„ë£Œ")
            return result

        except Exception as e:
            logger.error(f"âŒ í”¼ë“œë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "feedback_recorded": False,
                "training_performed": False,
            }

    def _save_user_lora_if_needed(self, user_id: str, user_profile: UserEmotionProfile):
        """í•„ìš”ì‹œ ì‚¬ìš©ì LoRA ì–´ëŒ‘í„° ì €ì¥"""
        try:
            if self.pipeline and hasattr(self.pipeline, "unet"):
                # í˜„ì¬ ëª¨ë¸ ìƒíƒœë¥¼ LoRAë¡œ ì €ì¥
                model_state = {
                    "unet_state_dict": self.pipeline.unet.state_dict(),
                    "user_preferences": user_profile.preference_weights,
                    "training_metadata": user_profile.learning_metadata,
                }

                self.lora_manager.save_user_lora(user_id, model_state)
                logger.info(f"ğŸ’¾ ì‚¬ìš©ì {user_id} LoRA ì–´ëŒ‘í„° ì €ì¥")
        except Exception as e:
            logger.warning(f"âš ï¸ LoRA ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_user_insights(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì¹˜ë£Œ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        user_profile = self.get_user_profile(user_id)
        return user_profile.get_therapeutic_insights()

    def get_emotion_history(self, user_id: str, limit: int = 10) -> List[Dict]:
        """ì‚¬ìš©ì ê°ì • íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        user_profile = self.get_user_profile(user_id)
        return user_profile.emotion_history[-limit:]

    def cleanup_old_images(self, days_old: int = 30):
        """ì˜¤ë˜ëœ ì´ë¯¸ì§€ íŒŒì¼ ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_old)
            cleaned_count = 0

            for image_file in self.output_dir.glob("*.png"):
                if image_file.stat().st_mtime < cutoff_date.timestamp():
                    image_file.unlink()
                    cleaned_count += 1

            logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ì´ë¯¸ì§€ {cleaned_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
            return cleaned_count

        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0


# =============================================================================
# CLI ì¸í„°í˜ì´ìŠ¤
# =============================================================================


def main():
    """ë©”ì¸ CLI ì¸í„°í˜ì´ìŠ¤"""

    parser = argparse.ArgumentParser(
        description="ê°ì • ê¸°ë°˜ ë””ì§€í„¸ ì¹˜ë£Œ ì´ë¯¸ì§€ ìƒì„± ì‹œìŠ¤í…œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python %(prog)s --user-id "alice" --text "ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ë‹¤" --prompt "ìì—° í’ê²½"
  python %(prog)s --user-id "bob" --text "ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ëŠ”ë‹¤" --feedback-score 4.2
  python %(prog)s --user-id "carol" --insights
        """,
    )

    # ê¸°ë³¸ ì¸ìë“¤
    parser.add_argument("--user-id", required=True, help="ì‚¬ìš©ì ID")
    parser.add_argument("--text", help="ê°ì • ì¼ê¸° í…ìŠ¤íŠ¸")
    parser.add_argument("--prompt", default="", help="ì¶”ê°€ í”„ë¡¬í”„íŠ¸")

    # ìƒì„± ì˜µì…˜ë“¤
    parser.add_argument("--steps", type=int, default=15, help="ì¶”ë¡  ìŠ¤í… ìˆ˜ (ê¸°ë³¸: 15)")
    parser.add_argument(
        "--guidance", type=float, default=7.5, help="ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ (ê¸°ë³¸: 7.5)"
    )
    parser.add_argument(
        "--width", type=int, default=512, help="ì´ë¯¸ì§€ ë„ˆë¹„ (ê¸°ë³¸: 512)"
    )
    parser.add_argument(
        "--height", type=int, default=512, help="ì´ë¯¸ì§€ ë†’ì´ (ê¸°ë³¸: 512)"
    )

    # í”¼ë“œë°± ì˜µì…˜ë“¤
    parser.add_argument("--feedback-score", type=float, help="í”¼ë“œë°± ì ìˆ˜ (1.0-5.0)")
    parser.add_argument("--emotion-id", type=int, help="í”¼ë“œë°±í•  ê°ì • ID")
    parser.add_argument("--comments", help="í”¼ë“œë°± ì½”ë©˜íŠ¸")
    parser.add_argument(
        "--no-training", action="store_true", help="í”¼ë“œë°± ì‹œ í•™ìŠµ ë¹„í™œì„±í™”"
    )

    # ì¡°íšŒ ì˜µì…˜ë“¤
    parser.add_argument("--insights", action="store_true", help="ì¹˜ë£Œ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ")
    parser.add_argument("--history", type=int, help="ê°ì • íˆìŠ¤í† ë¦¬ ì¡°íšŒ (ê°œìˆ˜)")

    # ì‹œìŠ¤í…œ ì˜µì…˜ë“¤
    parser.add_argument(
        "--model", default="runwayml/stable-diffusion-v1-5", help="ëª¨ë¸ ê²½ë¡œ"
    )
    parser.add_argument("--cleanup", type=int, help="ì˜¤ë˜ëœ ì´ë¯¸ì§€ ì •ë¦¬ (ì¼ ìˆ˜)")
    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")

    args = parser.parse_args()

    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("ğŸš€ ê°ì • ê¸°ë°˜ ë””ì§€í„¸ ì¹˜ë£Œ ì‹œìŠ¤í…œ ì‹œì‘")
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    print("-" * 60)

    try:
        system = EmotionalImageTherapySystem(model_path=args.model)

        # 1. ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ
        if args.text:
            print(f"ğŸ‘¤ ì‚¬ìš©ì: {args.user_id}")
            print(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {args.text}")
            print(f"ğŸ¨ í”„ë¡¬í”„íŠ¸: {args.prompt}")
            print()

            result = system.generate_therapeutic_image(
                user_id=args.user_id,
                input_text=args.text,
                base_prompt=args.prompt,
                num_inference_steps=args.steps,
                guidance_scale=args.guidance,
                width=args.width,
                height=args.height,
            )

            if result["success"]:
                metadata = result["metadata"]
                emotion = metadata["emotion"]

                print("âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!")
                print(
                    f"ğŸ˜Š ê°ì • ë¶„ì„: V={emotion['valence']:.3f}, A={emotion['arousal']:.3f}, D={emotion['dominance']:.3f}"
                )
                print(f"ğŸ¯ ìµœì¢… í”„ë¡¬í”„íŠ¸: {metadata['final_prompt']}")
                print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {metadata['image_path']}")
                print(f"ğŸ†” ê°ì • ID: {metadata['emotion_id']} (í”¼ë“œë°±ìš©)")
                print()

                # ì´ë¯¸ì§€ í‘œì‹œ (ê°€ëŠ¥í•œ ê²½ìš°)
                try:
                    import subprocess
                    import platform

                    if platform.system() == "Darwin":  # macOS
                        subprocess.run(["open", metadata["image_path"]], check=False)
                        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ë¥¼ ê¸°ë³¸ ë·°ì–´ë¡œ ì—´ì—ˆìŠµë‹ˆë‹¤.")
                    elif platform.system() == "Windows":
                        subprocess.run(
                            ["start", metadata["image_path"]], shell=True, check=False
                        )
                        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ë¥¼ ê¸°ë³¸ ë·°ì–´ë¡œ ì—´ì—ˆìŠµë‹ˆë‹¤.")
                    elif platform.system() == "Linux":
                        subprocess.run(
                            ["xdg-open", metadata["image_path"]], check=False
                        )
                        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ë¥¼ ê¸°ë³¸ ë·°ì–´ë¡œ ì—´ì—ˆìŠµë‹ˆë‹¤.")
                except Exception:
                    print("ğŸ’¡ ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•˜ë ¤ë©´ ìœ„ ê²½ë¡œë¥¼ ì—´ì–´ë³´ì„¸ìš”.")

            else:
                print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {result['error']}")
                return 1

        # 2. í”¼ë“œë°± ëª¨ë“œ
        elif args.feedback_score is not None:
            if args.emotion_id is None:
                print("âŒ í”¼ë“œë°±ì„ ìœ„í•´ì„œëŠ” --emotion-idê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return 1

            print(f"ğŸ‘¤ ì‚¬ìš©ì: {args.user_id}")
            print(f"ğŸ†” ê°ì • ID: {args.emotion_id}")
            print(f"â­ í”¼ë“œë°± ì ìˆ˜: {args.feedback_score}")
            if args.comments:
                print(f"ğŸ’¬ ì½”ë©˜íŠ¸: {args.comments}")
            print()

            result = system.process_feedback(
                user_id=args.user_id,
                emotion_id=args.emotion_id,
                feedback_score=args.feedback_score,
                comments=args.comments,
                enable_training=not args.no_training,
            )

            if result["success"]:
                print("âœ… í”¼ë“œë°± ì²˜ë¦¬ ì™„ë£Œ!")
                print(f"ğŸ“Š ì´ ìƒí˜¸ì‘ìš©: {result['total_interactions']}íšŒ")
                print(f"ğŸ“ ì´ í”¼ë“œë°±: {result['total_feedbacks']}íšŒ")

                if result["training_performed"]:
                    training_result = result["training_result"]
                    if "total_reward" in training_result:
                        print(
                            f"ğŸ¤– ê°œì¸í™” í•™ìŠµ ì™„ë£Œ: ë³´ìƒ {training_result['total_reward']:.3f}"
                        )
                    else:
                        print(
                            f"ğŸ¤– ê°œì¸í™” í•™ìŠµ ì™„ë£Œ: {training_result.get('mode', 'unknown')}"
                        )
                else:
                    print("â„¹ï¸ í•™ìŠµì€ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                # ê°„ë‹¨í•œ ì¸ì‚¬ì´íŠ¸ í‘œì‹œ
                insights = result["therapeutic_insights"]
                if "emotional_state" in insights:
                    mood = insights["emotional_state"]["current_mood"]
                    trend = insights["emotional_state"]["mood_trend"]
                    print(f"ğŸ˜Š í˜„ì¬ ê¸°ë¶„: {mood}")
                    print(f"ğŸ“ˆ ê¸°ë¶„ íŠ¸ë Œë“œ: {trend:+.3f}")

            else:
                print(f"âŒ í”¼ë“œë°± ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
                return 1

        # 3. ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ ëª¨ë“œ
        elif args.insights:
            print(f"ğŸ‘¤ ì‚¬ìš©ì: {args.user_id}")
            print("ğŸ“Š ì¹˜ë£Œ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ")
            print("-" * 40)

            insights = system.get_user_insights(args.user_id)

            if insights.get("status") == "insufficient_data":
                print("â„¹ï¸ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ ë” ë§ì€ ê°ì • ì¼ê¸°ë¥¼ ì‘ì„±í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
            else:
                # ê°ì • ìƒíƒœ
                emotional_state = insights["emotional_state"]
                print(f"ğŸ˜Š í˜„ì¬ ê¸°ë¶„: {emotional_state['current_mood']}")
                print(f"ğŸ“ˆ ê¸°ë¶„ íŠ¸ë Œë“œ: {emotional_state['mood_trend']:+.3f}")
                print(f"ğŸ¯ ê°ì • ì•ˆì •ì„±: {emotional_state['stability']:.3f}")
                print()

                # ì§„í–‰ ì§€í‘œ
                progress = insights["progress_indicators"]
                print("ğŸ“Š ì§„í–‰ ì§€í‘œ:")
                print(f"  â€¢ ì°¸ì—¬ë„: {progress['engagement_level']:.1%}")
                print(f"  â€¢ íšŒë³µ ì§€í‘œ: {progress['recovery_indicator']:.3f}")
                print(f"  â€¢ ì´ ìƒí˜¸ì‘ìš©: {progress['total_interactions']}íšŒ")
                print(f"  â€¢ í”¼ë“œë°± ìˆ˜: {progress['feedback_count']}íšŒ")
                print()

                # ì¶”ì²œì‚¬í•­
                recommendations = insights["recommendations"]
                print("ğŸ’¡ ì¶”ì²œì‚¬í•­:")
                for i, rec in enumerate(recommendations, 1):
                    print(f"  {i}. {rec}")
                print()

                # ê°œì¸í™” ì„ í˜¸ë„
                preferences = insights["preference_summary"]
                print("ğŸ¨ ê°œì¸í™” ì„ í˜¸ë„:")
                for key, value in preferences.items():
                    if isinstance(value, (int, float)):
                        print(f"  â€¢ {key}: {value:+.2f}")
                    else:
                        print(f"  â€¢ {key}: {value}")

        # 4. íˆìŠ¤í† ë¦¬ ì¡°íšŒ ëª¨ë“œ
        elif args.history is not None:
            print(f"ğŸ‘¤ ì‚¬ìš©ì: {args.user_id}")
            print(f"ğŸ“š ìµœê·¼ {args.history}ê°œ ê°ì • íˆìŠ¤í† ë¦¬")
            print("-" * 60)

            history = system.get_emotion_history(args.user_id, args.history)

            if not history:
                print("â„¹ï¸ ê°ì • íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                for i, record in enumerate(reversed(history), 1):
                    emotion = record["emotion"]
                    timestamp = record["timestamp"][:19].replace("T", " ")

                    print(f"[{i}] {timestamp}")
                    print(
                        f"    ğŸ“ ì…ë ¥: {record['input_text'][:60]}{'...' if len(record['input_text']) > 60 else ''}"
                    )
                    print(
                        f"    ğŸ˜Š ê°ì •: V={emotion.valence:.2f}, A={emotion.arousal:.2f}, D={emotion.dominance:.2f}"
                    )
                    if record.get("image_path"):
                        print(f"    ğŸ–¼ï¸ ì´ë¯¸ì§€: {record['image_path']}")
                    print(f"    ğŸ†” ID: {record.get('id', 'N/A')}")
                    print()

        # 5. ì •ë¦¬ ëª¨ë“œ
        elif args.cleanup is not None:
            print(f"ğŸ§¹ {args.cleanup}ì¼ ì´ìƒ ëœ ì´ë¯¸ì§€ íŒŒì¼ ì •ë¦¬")
            print("-" * 40)

            cleaned_count = system.cleanup_old_images(args.cleanup)
            print(f"âœ… {cleaned_count}ê°œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")

        # 6. ë„ì›€ë§ (ì¸ìê°€ ì—†ëŠ” ê²½ìš°)
        else:
            print("â“ ì‚¬ìš©ë²•:")
            print()
            print("1. ì´ë¯¸ì§€ ìƒì„±:")
            print('   python script.py --user-id "alice" --text "ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ë‹¤"')
            print()
            print("2. í”¼ë“œë°± ì œê³µ:")
            print(
                '   python script.py --user-id "alice" --emotion-id 1 --feedback-score 4.5'
            )
            print()
            print("3. ì¹˜ë£Œ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ:")
            print('   python script.py --user-id "alice" --insights')
            print()
            print("4. íˆìŠ¤í† ë¦¬ ì¡°íšŒ:")
            print('   python script.py --user-id "alice" --history 5')
            print()
            print("5. ë„ì›€ë§:")
            print("   python script.py --help")
            print()
            print("ğŸ’¡ ìì„¸í•œ ì˜µì…˜ì€ --helpë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.")

    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        return 130
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        if args.verbose:
            import traceback

            traceback.print_exc()
        return 1

    print("-" * 60)
    print("âœ… ì‘ì—… ì™„ë£Œ")
    return 0


# =============================================================================
# ì‹œìŠ¤í…œ ì •ë³´ ë° ìš”êµ¬ì‚¬í•­ ì²´í¬
# =============================================================================


def check_system_requirements():
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ìƒíƒœ í™•ì¸"""

    print("ğŸ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸")
    print("=" * 50)

    # Python ë²„ì „
    python_version = sys.version_info
    print(
        f"ğŸ Python: {python_version.major}.{python_version.minor}.{python_version.micro}"
    )

    # ë””ë°”ì´ìŠ¤ ì •ë³´
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device} ({device_type})")

    if device.type == "mps":
        print("ğŸ Apple Silicon ìµœì í™” í™œì„±í™”")
    elif device.type == "cuda":
        print(f"ğŸš€ CUDA ê°€ëŠ¥ (GPU: {torch.cuda.get_device_name()})")
    else:
        print("ğŸ’» CPU ëª¨ë“œ")

    # ë©”ëª¨ë¦¬ ì •ë³´
    if device.type == "mps":
        print("ğŸ’¾ í†µí•© ë©”ëª¨ë¦¬ (Apple Silicon)")
    elif device.type == "cuda":
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")

    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ
    print("\nğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ:")
    libraries = {
        "PyTorch": torch.__version__,
        "Transformers": TRANSFORMERS_AVAILABLE,
        "Diffusers": DIFFUSERS_AVAILABLE,
        "PEFT": PEFT_AVAILABLE,
        "PIL": True,  # ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
        "NumPy": np.__version__,
    }

    for lib, status in libraries.items():
        if isinstance(status, bool):
            status_str = "âœ… ì„¤ì¹˜ë¨" if status else "âŒ ë¯¸ì„¤ì¹˜"
        else:
            status_str = f"âœ… v{status}"
        print(f"  â€¢ {lib}: {status_str}")

    # ì„¤ì¹˜ ê¶Œì¥ì‚¬í•­
    missing_libs = []
    if not TRANSFORMERS_AVAILABLE:
        missing_libs.append("transformers")
    if not DIFFUSERS_AVAILABLE:
        missing_libs.append("diffusers")
    if not PEFT_AVAILABLE:
        missing_libs.append("peft")

    if missing_libs:
        print(f"\nâš ï¸ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬: {', '.join(missing_libs)}")
        print("ì„¤ì¹˜ ëª…ë ¹ì–´:")
        print(f"pip install {' '.join(missing_libs)}")
    else:
        print("\nâœ… ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")

    # ë””ë ‰í† ë¦¬ ìƒíƒœ
    print(f"\nğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬:")
    dirs_to_check = ["generated_images", "user_loras"]
    for dir_name in dirs_to_check:
        dir_path = Path(dir_name)
        if dir_path.exists():
            file_count = len(list(dir_path.glob("*")))
            print(f"  â€¢ {dir_name}/: âœ… ({file_count}ê°œ íŒŒì¼)")
        else:
            print(f"  â€¢ {dir_name}/: ğŸ“ ìƒì„± ì˜ˆì •")

    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
    db_path = Path("user_profiles.db")
    if db_path.exists():
        size_mb = db_path.stat().st_size / 1024 / 1024
        print(f"  â€¢ user_profiles.db: âœ… ({size_mb:.2f}MB)")
    else:
        print(f"  â€¢ user_profiles.db: ğŸ“„ ìƒì„± ì˜ˆì •")

    print("=" * 50)


def show_usage_examples():
    """ì‚¬ìš© ì˜ˆì‹œ í‘œì‹œ"""

    print("ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ")
    print("=" * 50)

    examples = [
        {
            "title": "1. ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±",
            "command": 'python emotion_therapy.py --user-id "alice" --text "ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ í–‰ë³µí–ˆë‹¤"',
            "description": "ì‚¬ìš©ìì˜ ê°ì • ì¼ê¸°ë¥¼ ë¶„ì„í•˜ì—¬ ì¹˜ë£Œìš© ì´ë¯¸ì§€ ìƒì„±",
        },
        {
            "title": "2. ìƒì„¸ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ìƒì„±",
            "command": 'python emotion_therapy.py --user-id "bob" --text "ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ì‹¬í•˜ë‹¤" --prompt "í‰ì˜¨í•œ ìì—° í’ê²½"',
            "description": "ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ì™€ ê°ì • ë¶„ì„ì„ ê²°í•©í•œ ë§ì¶¤í˜• ì´ë¯¸ì§€ ìƒì„±",
        },
        {
            "title": "3. ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±",
            "command": 'python emotion_therapy.py --user-id "carol" --text "ìš°ìš¸í•œ ê¸°ë¶„" --steps 25 --guidance 8.0',
            "description": "ë” ë§ì€ ì¶”ë¡  ìŠ¤í…ê³¼ ë†’ì€ ê°€ì´ë˜ìŠ¤ë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±",
        },
        {
            "title": "4. í”¼ë“œë°± ì œê³µ (ê¸ì •ì )",
            "command": 'python emotion_therapy.py --user-id "alice" --emotion-id 1 --feedback-score 4.8 --comments "ì •ë§ ë§ˆìŒì— ë“ ë‹¤"',
            "description": "ìƒì„±ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ê¸ì •ì  í”¼ë“œë°±ìœ¼ë¡œ ê°œì¸í™” í•™ìŠµ",
        },
        {
            "title": "5. í”¼ë“œë°± ì œê³µ (ê°œì„  í•„ìš”)",
            "command": 'python emotion_therapy.py --user-id "bob" --emotion-id 2 --feedback-score 2.3',
            "description": "ë¶€ì •ì  í”¼ë“œë°±ì„ í†µí•œ ëª¨ë¸ ê°œì„ ",
        },
        {
            "title": "6. ì¹˜ë£Œ ì§„í–‰ë„ í™•ì¸",
            "command": 'python emotion_therapy.py --user-id "alice" --insights',
            "description": "ê°ì • ìƒíƒœ, ì¹˜ë£Œ ì§„í–‰ë„, ê°œì¸í™” ì„ í˜¸ë„ ë“± ì¢…í•© ë¶„ì„",
        },
        {
            "title": "7. ê°ì • íˆìŠ¤í† ë¦¬ ì¡°íšŒ",
            "command": 'python emotion_therapy.py --user-id "carol" --history 10',
            "description": "ìµœê·¼ 10ê°œì˜ ê°ì • ê¸°ë¡ê³¼ ìƒì„±ëœ ì´ë¯¸ì§€ ì´ë ¥ í™•ì¸",
        },
        {
            "title": "8. ì‹œìŠ¤í…œ ì •ë¦¬",
            "command": 'python emotion_therapy.py --user-id "admin" --cleanup 30',
            "description": "30ì¼ ì´ìƒ ëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ì •ë¦¬í•˜ì—¬ ì €ì¥ ê³µê°„ í™•ë³´",
        },
    ]

    for example in examples:
        print(f"\n{example['title']}")
        print(f"ğŸ’» {example['command']}")
        print(f"ğŸ“ {example['description']}")

    print("\n" + "=" * 50)
    print("ğŸ”§ ê³ ê¸‰ ì˜µì…˜:")
    print("  --verbose          : ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥")
    print("  --no-training      : í”¼ë“œë°± ì‹œ í•™ìŠµ ë¹„í™œì„±í™”")
    print("  --model MODEL_PATH : ì‚¬ìš©í•  Stable Diffusion ëª¨ë¸ ì§€ì •")
    print("  --width WIDTH      : ì´ë¯¸ì§€ ë„ˆë¹„ (ê¸°ë³¸: 512)")
    print("  --height HEIGHT    : ì´ë¯¸ì§€ ë†’ì´ (ê¸°ë³¸: 512)")
    print("=" * 50)


# =============================================================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

if __name__ == "__main__":
    # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ (verbose ëª¨ë“œê±°ë‚˜ ë„ì›€ë§ì¸ ê²½ìš°)
    if len(sys.argv) == 1 or "--help" in sys.argv or "-h" in sys.argv:
        check_system_requirements()
        print()
        show_usage_examples()
        print()

    # ë©”ì¸ í”„ë¡œê·¸ë¨ ì‹¤í–‰
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        sys.exit(1)
