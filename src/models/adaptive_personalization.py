#!/usr/bin/env python3
"""
ì ì‘í˜• ê°œì¸í™” ì‹œìŠ¤í…œ - ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‚¬ìš©ì ì„ í˜¸ë„ í•™ìŠµ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import json
from datetime import datetime
import warnings

from config import device, logger
from models.emotion import EmotionEmbedding

warnings.filterwarnings("ignore", category=UserWarning)


@dataclass
class FeedbackContext:
    """í”¼ë“œë°± ì»¨í…ìŠ¤íŠ¸ ì •ë³´"""
    emotion: EmotionEmbedding
    image_features: Dict[str, float]  # ì´ë¯¸ì§€ íŠ¹ì„± (ë°ê¸°, ì±„ë„, ëŒ€ë¹„ ë“±)
    prompt_features: Dict[str, Any]   # í”„ë¡¬í”„íŠ¸ íŠ¹ì„±
    user_state: Dict[str, float]      # ì‚¬ìš©ì ìƒíƒœ (ì‹œê°„ëŒ€, ê¸°ë¶„ ì´ë ¥ ë“±)
    feedback_score: float
    timestamp: datetime


class AdaptivePreferenceNet(nn.Module):
    """ì ì‘í˜• ì„ í˜¸ë„ ì˜ˆì¸¡ ì‹ ê²½ë§"""
    
    def __init__(
        self,
        emotion_dim: int = 3,        # VAD ì°¨ì›
        image_feature_dim: int = 8,  # ì´ë¯¸ì§€ íŠ¹ì„± ì°¨ì›
        prompt_feature_dim: int = 10, # í”„ë¡¬í”„íŠ¸ íŠ¹ì„± ì°¨ì›
        user_state_dim: int = 5,     # ì‚¬ìš©ì ìƒíƒœ ì°¨ì›
        hidden_dim: int = 128,
        preference_dim: int = 7      # ì„ í˜¸ë„ ì°¨ì› ìˆ˜
    ):
        super().__init__()
        
        self.emotion_dim = emotion_dim
        self.image_feature_dim = image_feature_dim
        self.prompt_feature_dim = prompt_feature_dim
        self.user_state_dim = user_state_dim
        self.preference_dim = preference_dim
        
        # ì…ë ¥ ì°¨ì› ê³„ì‚°
        input_dim = emotion_dim + image_feature_dim + prompt_feature_dim + user_state_dim
        
        # ì‹ ê²½ë§ êµ¬ì¡°
        self.feature_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # ì„ í˜¸ë„ ì˜ˆì¸¡ í—¤ë“œ
        self.preference_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, preference_dim),
            nn.Tanh()  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
        )
        
        # ë§Œì¡±ë„ ì˜ˆì¸¡ í—¤ë“œ (í”¼ë“œë°± ì ìˆ˜ ì˜ˆì¸¡)
        self.satisfaction_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()  # [0, 1] ë²”ìœ„
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, context_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            context_features: [batch_size, input_dim] ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„±
            
        Returns:
            preferences: [batch_size, preference_dim] ì˜ˆì¸¡ëœ ì„ í˜¸ë„
            satisfaction: [batch_size, 1] ì˜ˆì¸¡ëœ ë§Œì¡±ë„
        """
        features = self.feature_encoder(context_features)
        
        preferences = self.preference_head(features)
        satisfaction = self.satisfaction_head(features)
        
        return preferences, satisfaction


class ContextualBandit:
    """ì»¨í…ìŠ¤ì¶”ì–¼ ë°´ë”§ ê¸°ë°˜ ì„ í˜¸ë„ íƒìƒ‰"""
    
    def __init__(self, preference_dim: int = 7, exploration_rate: float = 0.1):
        self.preference_dim = preference_dim
        self.exploration_rate = exploration_rate
        self.action_counts = np.zeros(preference_dim)
        self.action_rewards = np.zeros(preference_dim)
        
    def select_exploration_direction(self, current_preferences: np.ndarray) -> np.ndarray:
        """íƒìƒ‰ ë°©í–¥ ì„ íƒ (UCB ê¸°ë°˜)"""
        
        total_counts = np.sum(self.action_counts) + 1
        
        # UCB (Upper Confidence Bound) ê³„ì‚°
        confidence_intervals = np.sqrt(
            2 * np.log(total_counts) / (self.action_counts + 1)
        )
        
        # í‰ê·  ë³´ìƒ + ì‹ ë¢°ë„ êµ¬ê°„
        ucb_values = (self.action_rewards / (self.action_counts + 1)) + confidence_intervals
        
        # ê°€ì¥ ë†’ì€ UCB ê°’ì„ ê°€ì§„ ì°¨ì› ì„ íƒ
        explore_dim = np.argmax(ucb_values)
        
        # íƒìƒ‰ ë°©í–¥ ìƒì„±
        exploration_vector = np.zeros(self.preference_dim)
        exploration_vector[explore_dim] = np.random.choice([-1, 1]) * self.exploration_rate
        
        return current_preferences + exploration_vector
    
    def update_rewards(self, action_dim: int, reward: float):
        """ë³´ìƒ ì—…ë°ì´íŠ¸"""
        self.action_counts[action_dim] += 1
        self.action_rewards[action_dim] += reward


class AdaptivePersonalizationSystem:
    """ì ì‘í˜• ê°œì¸í™” ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        learning_rate: float = 1e-3,
        update_interval: int = 5,  # ëª‡ ë²ˆì˜ í”¼ë“œë°±ë§ˆë‹¤ ëª¨ë¸ ì—…ë°ì´íŠ¸
        exploration_rate: float = 0.1
    ):
        self.learning_rate = learning_rate
        self.update_interval = update_interval
        self.exploration_rate = exploration_rate
        
        # ì ì‘í˜• ì„ í˜¸ë„ ì‹ ê²½ë§
        self.preference_net = AdaptivePreferenceNet().to(device)
        self.optimizer = optim.Adam(self.preference_net.parameters(), lr=learning_rate)
        
        # ì»¨í…ìŠ¤ì¶”ì–¼ ë°´ë”§
        self.bandit = ContextualBandit(exploration_rate=exploration_rate)
        
        # í•™ìŠµ ë°ì´í„° ë²„í¼
        self.feedback_buffer: List[FeedbackContext] = []
        self.max_buffer_size = 1000
        
        # ê°œì¸ë³„ ì„ í˜¸ë„ ì €ì¥
        self.user_preferences: Dict[str, np.ndarray] = {}
        self.user_contexts: Dict[str, List[FeedbackContext]] = {}
        
        # í•™ìŠµ ë©”íŠ¸ë¦­
        self.training_history = {
            "losses": [],
            "accuracies": [],
            "exploration_rates": []
        }
        
        # ì„ í˜¸ë„ ì°¨ì› ì •ì˜
        self.preference_names = [
            "color_temperature", "brightness", "saturation", 
            "contrast", "complexity", "artistic_style", "composition"
        ]
        
        logger.info("âœ… ì ì‘í˜• ê°œì¸í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_context_features(
        self, 
        emotion: EmotionEmbedding,
        image_metadata: Dict[str, Any],
        prompt: str,
        user_id: str,
        timestamp: datetime = None
    ) -> Dict[str, float]:
        """ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ"""
        
        if timestamp is None:
            timestamp = datetime.now()
        
        # 1. ê°ì • íŠ¹ì„±
        emotion_features = {
            "valence": emotion.valence,
            "arousal": emotion.arousal,
            "dominance": emotion.dominance
        }
        
        # 2. ì´ë¯¸ì§€ íŠ¹ì„± (ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ)
        image_features = {
            "brightness": image_metadata.get("brightness", 0.0),
            "saturation": image_metadata.get("saturation", 0.0),
            "contrast": image_metadata.get("contrast", 0.0),
            "hue_variance": image_metadata.get("hue_variance", 0.0),
            "edge_density": image_metadata.get("edge_density", 0.0),
            "color_diversity": image_metadata.get("color_diversity", 0.0),
            "composition_balance": image_metadata.get("composition_balance", 0.0),
            "texture_complexity": image_metadata.get("texture_complexity", 0.0)
        }
        
        # 3. í”„ë¡¬í”„íŠ¸ íŠ¹ì„±
        prompt_features = {
            "length": len(prompt.split()) / 50.0,  # ì •ê·œí™”
            "positive_words": self._count_positive_words(prompt),
            "negative_words": self._count_negative_words(prompt),
            "color_mentions": self._count_color_mentions(prompt),
            "emotion_words": self._count_emotion_words(prompt),
            "art_style_mentions": self._count_art_style_mentions(prompt),
            "complexity_indicators": self._count_complexity_indicators(prompt),
            "nature_elements": self._count_nature_elements(prompt),
            "human_elements": self._count_human_elements(prompt),
            "abstract_elements": self._count_abstract_elements(prompt)
        }
        
        # 4. ì‚¬ìš©ì ìƒíƒœ íŠ¹ì„±
        hour = timestamp.hour
        user_state_features = {
            "time_of_day": np.sin(2 * np.pi * hour / 24),  # ì‹œê°„ì„ ìˆœí™˜ íŠ¹ì„±ìœ¼ë¡œ
            "time_of_day_cos": np.cos(2 * np.pi * hour / 24),
            "recent_mood_trend": self._calculate_mood_trend(user_id),
            "session_length": self._get_session_length(user_id),
            "interaction_frequency": self._get_interaction_frequency(user_id)
        }
        
        return {
            **emotion_features,
            **image_features, 
            **prompt_features,
            **user_state_features
        }
    
    def _count_positive_words(self, prompt: str) -> float:
        """ê¸ì •ì  ë‹¨ì–´ ê°œìˆ˜"""
        positive_words = {
            "beautiful", "bright", "cheerful", "happy", "joyful", "peaceful",
            "vibrant", "warm", "wonderful", "amazing", "stunning", "lovely"
        }
        words = prompt.lower().split()
        return sum(1 for word in words if word in positive_words) / len(words)
    
    def _count_negative_words(self, prompt: str) -> float:
        """ë¶€ì •ì  ë‹¨ì–´ ê°œìˆ˜"""
        negative_words = {
            "dark", "sad", "gloomy", "depressing", "cold", "harsh",
            "ugly", "terrible", "awful", "nightmare", "scary", "disturbing"
        }
        words = prompt.lower().split()
        return sum(1 for word in words if word in negative_words) / len(words)
    
    def _count_color_mentions(self, prompt: str) -> float:
        """ìƒ‰ìƒ ì–¸ê¸‰ ê°œìˆ˜"""
        color_words = {
            "red", "blue", "green", "yellow", "purple", "orange", "pink",
            "black", "white", "brown", "gray", "violet", "cyan", "magenta"
        }
        words = prompt.lower().split()
        return sum(1 for word in words if word in color_words) / len(words)
    
    def _count_emotion_words(self, prompt: str) -> float:
        """ê°ì • ë‹¨ì–´ ê°œìˆ˜"""
        emotion_words = {
            "calm", "excited", "relaxed", "energetic", "serene", "dynamic",
            "peaceful", "lively", "tranquil", "passionate", "soothing", "intense"
        }
        words = prompt.lower().split()
        return sum(1 for word in words if word in emotion_words) / len(words)
    
    def _count_art_style_mentions(self, prompt: str) -> float:
        """ì˜ˆìˆ  ìŠ¤íƒ€ì¼ ì–¸ê¸‰ ê°œìˆ˜"""
        art_styles = {
            "realistic", "abstract", "impressionist", "minimalist", "surreal",
            "watercolor", "oil painting", "digital art", "sketch", "vintage"
        }
        words = prompt.lower().split()
        return sum(1 for word in words if word in art_styles) / len(words)
    
    def _count_complexity_indicators(self, prompt: str) -> float:
        """ë³µì¡ì„± ì§€ì‹œì–´ ê°œìˆ˜"""
        complexity_words = {
            "detailed", "intricate", "complex", "elaborate", "simple", 
            "minimalist", "clean", "busy", "cluttered", "sparse"
        }
        words = prompt.lower().split()
        return sum(1 for word in words if word in complexity_words) / len(words)
    
    def _count_nature_elements(self, prompt: str) -> float:
        """ìì—° ìš”ì†Œ ê°œìˆ˜"""
        nature_words = {
            "forest", "mountain", "ocean", "sky", "tree", "flower",
            "landscape", "nature", "garden", "beach", "sunset", "sunrise"
        }
        words = prompt.lower().split()
        return sum(1 for word in words if word in nature_words) / len(words)
    
    def _count_human_elements(self, prompt: str) -> float:
        """ì¸ê°„ ìš”ì†Œ ê°œìˆ˜"""
        human_words = {
            "person", "people", "human", "face", "portrait", "family",
            "child", "woman", "man", "smile", "emotion", "expression"
        }
        words = prompt.lower().split()
        return sum(1 for word in words if word in human_words) / len(words)
    
    def _count_abstract_elements(self, prompt: str) -> float:
        """ì¶”ìƒì  ìš”ì†Œ ê°œìˆ˜"""
        abstract_words = {
            "abstract", "geometric", "pattern", "shape", "form", "concept",
            "idea", "emotion", "feeling", "mood", "atmosphere", "energy"
        }
        words = prompt.lower().split()
        return sum(1 for word in words if word in abstract_words) / len(words)
    
    def _calculate_mood_trend(self, user_id: str) -> float:
        """ìµœê·¼ ê¸°ë¶„ íŠ¸ë Œë“œ ê³„ì‚°"""
        if user_id not in self.user_contexts or len(self.user_contexts[user_id]) < 2:
            return 0.0
        
        recent_contexts = self.user_contexts[user_id][-10:]  # ìµœê·¼ 10ê°œ
        valences = [ctx.emotion.valence for ctx in recent_contexts]
        
        if len(valences) < 2:
            return 0.0
        
        # ì„ í˜• íŠ¸ë Œë“œ ê³„ì‚°
        x = np.arange(len(valences))
        slope = np.polyfit(x, valences, 1)[0]
        return float(slope)
    
    def _get_session_length(self, user_id: str) -> float:
        """í˜„ì¬ ì„¸ì…˜ ê¸¸ì´ (ì •ê·œí™”)"""
        if user_id not in self.user_contexts or not self.user_contexts[user_id]:
            return 0.0
        
        session_start = self.user_contexts[user_id][0].timestamp
        current_time = datetime.now()
        session_length = (current_time - session_start).total_seconds() / 3600  # ì‹œê°„ ë‹¨ìœ„
        
        return min(session_length / 2.0, 1.0)  # 2ì‹œê°„ì„ ìµœëŒ€ê°’ìœ¼ë¡œ ì •ê·œí™”
    
    def _get_interaction_frequency(self, user_id: str) -> float:
        """ìƒí˜¸ì‘ìš© ë¹ˆë„ (ì •ê·œí™”)"""
        if user_id not in self.user_contexts:
            return 0.0
        
        interaction_count = len(self.user_contexts[user_id])
        return min(interaction_count / 50.0, 1.0)  # 50íšŒë¥¼ ìµœëŒ€ê°’ìœ¼ë¡œ ì •ê·œí™”
    
    def add_feedback(
        self,
        user_id: str,
        emotion: EmotionEmbedding,
        image_metadata: Dict[str, Any],
        prompt: str,
        feedback_score: float,
        timestamp: datetime = None
    ):
        """í”¼ë“œë°± ì¶”ê°€ ë° í•™ìŠµ"""
        
        if timestamp is None:
            timestamp = datetime.now()
        
        # ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ
        context_features = self.extract_context_features(
            emotion, image_metadata, prompt, user_id, timestamp
        )
        
        # í”¼ë“œë°± ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        feedback_context = FeedbackContext(
            emotion=emotion,
            image_features={k: v for k, v in context_features.items() 
                          if k in ["brightness", "saturation", "contrast", "hue_variance", 
                                 "edge_density", "color_diversity", "composition_balance", "texture_complexity"]},
            prompt_features={k: v for k, v in context_features.items() 
                           if k in ["length", "positive_words", "negative_words", "color_mentions",
                                  "emotion_words", "art_style_mentions", "complexity_indicators",
                                  "nature_elements", "human_elements", "abstract_elements"]},
            user_state={k: v for k, v in context_features.items() 
                       if k in ["time_of_day", "time_of_day_cos", "recent_mood_trend",
                              "session_length", "interaction_frequency"]},
            feedback_score=feedback_score,
            timestamp=timestamp
        )
        
        # ì‚¬ìš©ìë³„ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
        if user_id not in self.user_contexts:
            self.user_contexts[user_id] = []
        self.user_contexts[user_id].append(feedback_context)
        
        # ì „ì—­ í”¼ë“œë°± ë²„í¼ì— ì¶”ê°€
        self.feedback_buffer.append(feedback_context)
        if len(self.feedback_buffer) > self.max_buffer_size:
            self.feedback_buffer.pop(0)
        
        # ì£¼ê¸°ì  ëª¨ë¸ ì—…ë°ì´íŠ¸
        if len(self.feedback_buffer) % self.update_interval == 0:
            self._update_model()
        
        # ì¦‰ì‹œ ì„ í˜¸ë„ ì—…ë°ì´íŠ¸
        self._update_user_preferences(user_id, feedback_context)
        
        logger.info(f"âœ… ì‚¬ìš©ì {user_id} í”¼ë“œë°± ì¶”ê°€ ë° í•™ìŠµ ì™„ë£Œ")
    
    def _update_model(self):
        """ì‹ ê²½ë§ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        
        if len(self.feedback_buffer) < 10:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ëŸ‰
            return
        
        self.preference_net.train()
        
        # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
        batch_contexts = []
        batch_preferences = []
        batch_satisfactions = []
        
        for feedback_ctx in self.feedback_buffer[-50:]:  # ìµœê·¼ 50ê°œ
            # ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„± ë²¡í„°í™”
            context_vector = self._vectorize_context(feedback_ctx)
            batch_contexts.append(context_vector)
            
            # íƒ€ê²Ÿ ì„ í˜¸ë„ (í˜„ì¬ëŠ” í”¼ë“œë°± ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©)
            target_prefs = self._feedback_to_preferences(feedback_ctx)
            batch_preferences.append(target_prefs)
            
            # ë§Œì¡±ë„ (ì •ê·œí™”ëœ í”¼ë“œë°± ì ìˆ˜)
            satisfaction = (feedback_ctx.feedback_score - 1) / 4  # 1-5 -> 0-1
            batch_satisfactions.append([satisfaction])
        
        # í…ì„œ ë³€í™˜
        contexts_tensor = torch.FloatTensor(batch_contexts).to(device)
        prefs_tensor = torch.FloatTensor(batch_preferences).to(device)
        sats_tensor = torch.FloatTensor(batch_satisfactions).to(device)
        
        # Forward pass
        pred_prefs, pred_sats = self.preference_net(contexts_tensor)
        
        # ì†ì‹¤ ê³„ì‚°
        pref_loss = nn.MSELoss()(pred_prefs, prefs_tensor)
        sat_loss = nn.MSELoss()(pred_sats, sats_tensor)
        total_loss = pref_loss + sat_loss
        
        # Backward pass
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.training_history["losses"].append(total_loss.item())
        
        logger.info(f"ğŸ¤– ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì†ì‹¤ {total_loss.item():.4f}")
    
    def _vectorize_context(self, feedback_ctx: FeedbackContext) -> List[float]:
        """í”¼ë“œë°± ì»¨í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        
        vector = []
        
        # ê°ì • íŠ¹ì„±
        vector.extend([
            feedback_ctx.emotion.valence,
            feedback_ctx.emotion.arousal, 
            feedback_ctx.emotion.dominance
        ])
        
        # ì´ë¯¸ì§€ íŠ¹ì„±
        for key in ["brightness", "saturation", "contrast", "hue_variance", 
                   "edge_density", "color_diversity", "composition_balance", "texture_complexity"]:
            vector.append(feedback_ctx.image_features.get(key, 0.0))
        
        # í”„ë¡¬í”„íŠ¸ íŠ¹ì„±
        for key in ["length", "positive_words", "negative_words", "color_mentions",
                   "emotion_words", "art_style_mentions", "complexity_indicators",
                   "nature_elements", "human_elements", "abstract_elements"]:
            vector.append(feedback_ctx.prompt_features.get(key, 0.0))
        
        # ì‚¬ìš©ì ìƒíƒœ
        for key in ["time_of_day", "time_of_day_cos", "recent_mood_trend",
                   "session_length", "interaction_frequency"]:
            vector.append(feedback_ctx.user_state.get(key, 0.0))
        
        return vector
    
    def _feedback_to_preferences(self, feedback_ctx: FeedbackContext) -> List[float]:
        """í”¼ë“œë°±ì„ ì„ í˜¸ë„ë¡œ ë³€í™˜ (ì´ˆê¸° íœ´ë¦¬ìŠ¤í‹±)"""
        
        preferences = [0.0] * len(self.preference_names)
        feedback_strength = (feedback_ctx.feedback_score - 3) / 2  # -1 to 1
        
        if abs(feedback_strength) < 0.1:  # ì¤‘ì„± í”¼ë“œë°±
            return preferences
        
        # ë‹¤ì–‘í•œ ìš”ì†Œë¥¼ ê³ ë ¤í•œ ë™ì  ë§¤í•‘
        emotion = feedback_ctx.emotion
        
        # ìƒ‰ì˜¨ë„ (ê°ì • valence + í”„ë¡¬í”„íŠ¸ íŠ¹ì„± ê¸°ë°˜)
        preferences[0] = feedback_strength * (
            0.3 * emotion.valence + 
            0.2 * feedback_ctx.prompt_features.get("positive_words", 0) -
            0.2 * feedback_ctx.prompt_features.get("negative_words", 0) +
            0.3 * (feedback_ctx.user_state.get("time_of_day", 0) > 0)  # ë‚® ì‹œê°„ ì„ í˜¸
        )
        
        # ë°ê¸° (arousal + ì‹œê°„ëŒ€ ê³ ë ¤)
        preferences[1] = feedback_strength * (
            0.4 * emotion.arousal +
            0.3 * feedback_ctx.prompt_features.get("positive_words", 0) +
            0.3 * np.cos(2 * np.pi * datetime.now().hour / 24)  # ë‚®ì— ë” ë°ê²Œ
        )
        
        # ì±„ë„ (arousal + ê°ì • ê°•ë„)
        preferences[2] = feedback_strength * (
            0.5 * emotion.arousal +
            0.3 * abs(emotion.valence) +
            0.2 * feedback_ctx.prompt_features.get("color_mentions", 0)
        )
        
        # ëŒ€ë¹„ (dominance + ë³µì¡ì„± ì„ í˜¸)
        preferences[3] = feedback_strength * (
            0.4 * emotion.dominance +
            0.3 * feedback_ctx.prompt_features.get("complexity_indicators", 0) +
            0.3 * emotion.arousal
        )
        
        # ë³µì¡ì„± (ê°œì¸ë³„ í•™ìŠµ ì´ë ¥ + í”„ë¡¬í”„íŠ¸)
        preferences[4] = feedback_strength * (
            0.3 * emotion.dominance +
            0.4 * feedback_ctx.prompt_features.get("complexity_indicators", 0) +
            0.3 * feedback_ctx.user_state.get("interaction_frequency", 0)
        )
        
        # ì˜ˆìˆ  ìŠ¤íƒ€ì¼ (í”„ë¡¬í”„íŠ¸ ê¸°ë°˜)
        preferences[5] = feedback_strength * (
            0.5 * feedback_ctx.prompt_features.get("art_style_mentions", 0) +
            0.3 * feedback_ctx.prompt_features.get("abstract_elements", 0) +
            0.2 * emotion.valence
        )
        
        # êµ¬ì„± (dominance + ê· í˜• ì„ í˜¸)
        preferences[6] = feedback_strength * (
            0.4 * emotion.dominance +
            0.3 * feedback_ctx.image_features.get("composition_balance", 0) +
            0.3 * feedback_ctx.prompt_features.get("nature_elements", 0)
        )
        
        # ë²”ìœ„ ì œí•œ
        preferences = [max(-1.0, min(1.0, p)) for p in preferences]
        
        return preferences
    
    def _update_user_preferences(self, user_id: str, feedback_ctx: FeedbackContext):
        """ì‚¬ìš©ìë³„ ì„ í˜¸ë„ ì—…ë°ì´íŠ¸"""
        
        if user_id not in self.user_preferences:
            self.user_preferences[user_id] = np.zeros(len(self.preference_names))
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì„ í˜¸ë„ ì˜ˆì¸¡
        if len(self.feedback_buffer) > 10:
            context_vector = torch.FloatTensor([self._vectorize_context(feedback_ctx)]).to(device)
            
            self.preference_net.eval()
            with torch.no_grad():
                predicted_prefs, _ = self.preference_net(context_vector)
                predicted_prefs = predicted_prefs.cpu().numpy()[0]
        else:
            # ì´ˆê¸°ì—ëŠ” íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
            predicted_prefs = np.array(self._feedback_to_preferences(feedback_ctx))
        
        # ì ì‘í˜• í•™ìŠµë¥  (í”¼ë“œë°± ê°•ë„ì— ë¹„ë¡€)
        feedback_strength = abs(feedback_ctx.feedback_score - 3) / 2
        adaptive_lr = 0.05 + 0.15 * feedback_strength  # 0.05 ~ 0.2
        
        # íƒìƒ‰-í™œìš© ê· í˜•
        if np.random.random() < self.exploration_rate:
            # íƒìƒ‰: ë°´ë”§ ê¸°ë°˜ íƒìƒ‰ ë°©í–¥
            exploration_prefs = self.bandit.select_exploration_direction(
                self.user_preferences[user_id]
            )
            self.user_preferences[user_id] = (
                (1 - adaptive_lr) * self.user_preferences[user_id] + 
                adaptive_lr * exploration_prefs
            )
        else:
            # í™œìš©: ì˜ˆì¸¡ëœ ì„ í˜¸ë„ ì‚¬ìš©
            self.user_preferences[user_id] = (
                (1 - adaptive_lr) * self.user_preferences[user_id] + 
                adaptive_lr * predicted_prefs
            )
        
        # ë°´ë”§ ë³´ìƒ ì—…ë°ì´íŠ¸
        for i, pref_val in enumerate(predicted_prefs):
            if abs(pref_val) > 0.1:  # ìœ ì˜ë¯¸í•œ ì„ í˜¸ë„ ë³€í™”
                reward = feedback_strength if pref_val * feedback_strength > 0 else -feedback_strength
                self.bandit.update_rewards(i, reward)
        
        # ë²”ìœ„ ì œí•œ
        self.user_preferences[user_id] = np.clip(self.user_preferences[user_id], -1.0, 1.0)
    
    def get_user_preferences(self, user_id: str) -> Dict[str, float]:
        """ì‚¬ìš©ì ì„ í˜¸ë„ ë°˜í™˜"""
        
        if user_id not in self.user_preferences:
            return {name: 0.0 for name in self.preference_names}
        
        prefs_array = self.user_preferences[user_id]
        return {
            name: float(prefs_array[i]) 
            for i, name in enumerate(self.preference_names)
        }
    
    def predict_satisfaction(
        self,
        user_id: str,
        emotion: EmotionEmbedding,
        image_metadata: Dict[str, Any],
        prompt: str
    ) -> float:
        """ë§Œì¡±ë„ ì˜ˆì¸¡"""
        
        context_features = self.extract_context_features(
            emotion, image_metadata, prompt, user_id
        )
        
        feedback_ctx = FeedbackContext(
            emotion=emotion,
            image_features={k: v for k, v in context_features.items() 
                          if k in ["brightness", "saturation", "contrast", "hue_variance", 
                                 "edge_density", "color_diversity", "composition_balance", "texture_complexity"]},
            prompt_features={k: v for k, v in context_features.items() 
                           if k in ["length", "positive_words", "negative_words", "color_mentions",
                                  "emotion_words", "art_style_mentions", "complexity_indicators",
                                  "nature_elements", "human_elements", "abstract_elements"]},
            user_state={k: v for k, v in context_features.items() 
                       if k in ["time_of_day", "time_of_day_cos", "recent_mood_trend",
                              "session_length", "interaction_frequency"]},
            feedback_score=3.0,  # ë”ë¯¸ê°’
            timestamp=datetime.now()
        )
        
        context_vector = torch.FloatTensor([self._vectorize_context(feedback_ctx)]).to(device)
        
        self.preference_net.eval()
        with torch.no_grad():
            _, predicted_satisfaction = self.preference_net(context_vector)
            satisfaction = predicted_satisfaction.cpu().item()
        
        return satisfaction * 4 + 1  # 0-1ì„ 1-5ë¡œ ë³€í™˜
    
    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        
        save_data = {
            "model_state_dict": self.preference_net.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "user_preferences": {k: v.tolist() for k, v in self.user_preferences.items()},
            "bandit_state": {
                "action_counts": self.bandit.action_counts.tolist(),
                "action_rewards": self.bandit.action_rewards.tolist()
            },
            "training_history": self.training_history,
            "preference_names": self.preference_names
        }
        
        torch.save(save_data, path)
        logger.info(f"âœ… ì ì‘í˜• ê°œì¸í™” ëª¨ë¸ ì €ì¥: {path}")
    
    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        
        try:
            save_data = torch.load(path, map_location=device)
            
            self.preference_net.load_state_dict(save_data["model_state_dict"])
            self.optimizer.load_state_dict(save_data["optimizer_state_dict"])
            
            self.user_preferences = {
                k: np.array(v) for k, v in save_data["user_preferences"].items()
            }
            
            self.bandit.action_counts = np.array(save_data["bandit_state"]["action_counts"])
            self.bandit.action_rewards = np.array(save_data["bandit_state"]["action_rewards"])
            
            self.training_history = save_data["training_history"]
            self.preference_names = save_data["preference_names"]
            
            logger.info(f"âœ… ì ì‘í˜• ê°œì¸í™” ëª¨ë¸ ë¡œë“œ: {path}")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_training_summary(self) -> Dict[str, Any]:
        """í•™ìŠµ ìš”ì•½ ì •ë³´"""
        
        summary = {
            "total_users": len(self.user_preferences),
            "total_feedback": len(self.feedback_buffer),
            "training_iterations": len(self.training_history["losses"]),
            "average_loss": np.mean(self.training_history["losses"][-10:]) if self.training_history["losses"] else 0.0,
            "exploration_rate": self.exploration_rate,
            "model_parameters": sum(p.numel() for p in self.preference_net.parameters()),
            "bandit_total_actions": np.sum(self.bandit.action_counts)
        }
        
        return summary