#!/usr/bin/env python3
"""
EmotionalImageTherapySystem - ê°ì • ê¸°ë°˜ ì´ë¯¸ì§€ ì¹˜ë£Œ ì‹œìŠ¤í…œ
"""

import os
import warnings
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union

import numpy as np
import torch
from PIL import Image

from config import (
    device,
    logger,
    TRANSFORMERS_AVAILABLE,
    DIFFUSERS_AVAILABLE,
    PEFT_AVAILABLE,
    GENERATED_IMAGES_DIR,
    USER_LORAS_DIR,
    DATABASE_NAME,
)
from models.emotion import EmotionEmbedding
from models.emotion_mapper import AdvancedEmotionMapper
from models.user_profile import UserEmotionProfile
from models.lora_manager import PersonalizedLoRAManager
from models.reward_model import DRaFTPlusRewardModel
from models.smart_feedback_system import FeedbackEnhancer
from training.trainer import DRaFTPlusTrainer

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# ì„ íƒì  ì„í¬íŠ¸
if DIFFUSERS_AVAILABLE:
    from diffusers import (
        StableDiffusionPipeline,
        EulerDiscreteScheduler,
    )


class EmotionalImageTherapySystem:
    """ê°ì • ê¸°ë°˜ ì´ë¯¸ì§€ ì¹˜ë£Œ ì‹œìŠ¤í…œ"""

    def __init__(self, model_path: str = "runwayml/stable-diffusion-v1-5"):
        self.model_path = model_path
        self.device = device

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = Path(GENERATED_IMAGES_DIR)
        self.output_dir.mkdir(exist_ok=True)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        logger.info("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")

        # 1. ê°ì • ë§¤í¼ ì´ˆê¸°í™”
        self.emotion_mapper = AdvancedEmotionMapper()

        # 2. LoRA ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.lora_manager = PersonalizedLoRAManager(model_path, lora_dir=USER_LORAS_DIR)

        # 3. SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        self.pipeline = self._load_pipeline()

        # 4. ë³´ìƒ ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        if self.pipeline:
            self.reward_model = DRaFTPlusRewardModel(self.device)
            
            # ê°œì„ ëœ íŠ¸ë ˆì´ë„ˆ ì„ íƒ (LoRA ë˜ëŠ” DDPO)
            try:
                from training.ddpo_trainer import ImprovedDRaFTPlusTrainer
                self.trainer = ImprovedDRaFTPlusTrainer(
                    self.pipeline, self.reward_model, use_lora=True
                )
                logger.info("âœ… ê°œì„ ëœ DDPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ ê°œì„ ëœ íŠ¸ë ˆì´ë„ˆ ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ íŠ¸ë ˆì´ë„ˆ ì‚¬ìš©")
                from training.trainer import DRaFTPlusTrainer
                self.trainer = DRaFTPlusTrainer(self.pipeline, self.reward_model)
        else:
            self.reward_model = None
            self.trainer = None

        # 5. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìºì‹œ
        self.user_profiles = {}
        
        # 6. ìŠ¤ë§ˆíŠ¸ í”¼ë“œë°± ì‹œìŠ¤í…œ
        self.feedback_enhancer = FeedbackEnhancer()

        logger.info("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")

    def _load_pipeline(self):
        """SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if not DIFFUSERS_AVAILABLE:
            logger.error("âŒ Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None

        try:
            logger.info(f"ğŸ“¦ Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘: {self.model_path}")

            pipeline = StableDiffusionPipeline.from_pretrained(
                self.model_path,
                torch_dtype=(
                    # ìœ íš¨í•˜ì§€ ì•Šì€ ìˆ«ìê°€ ë“¤ì–´ê°€ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë¯€ë¡œ ëª¨ë‘ float32ë¡œ ì„¤ì •
                    torch.float32
                    if self.device.type == "mps"
                    else torch.float32
                ),
                use_safetensors=True,
                safety_checker=None,  # ë¹ ë¥¸ ìƒì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
                requires_safety_checker=False,
            )

            # ìµœì í™” ì„¤ì •
            pipeline = pipeline.to(self.device)

            # ë©”ëª¨ë¦¬ ìµœì í™”
            pipeline.enable_attention_slicing()

            if self.device.type == "cuda":
                pipeline.enable_sequential_cpu_offload()

            # ë¹ ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ë³€ê²½
            pipeline.scheduler = EulerDiscreteScheduler.from_config(
                pipeline.scheduler.config
            )

            logger.info("âœ… SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ ë° ìµœì í™” ì™„ë£Œ")
            return pipeline

        except Exception as e:
            logger.error(f"âŒ SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_user_profile(self, user_id: str) -> UserEmotionProfile:
        """ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserEmotionProfile(user_id, db_path=DATABASE_NAME)
            logger.info(f"âœ… ìƒˆ ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìƒì„±: {user_id}")
        return self.user_profiles[user_id]

    def generate_therapeutic_image(
        self,
        user_id: str,
        input_text: str,
        base_prompt: str = "",
        num_inference_steps: int = 15,
        guidance_scale: float = 7.5,
        width: int = 512,
        height: int = 512,
    ) -> Dict[str, Any]:
        """ì¹˜ë£Œìš© ì´ë¯¸ì§€ ìƒì„±"""

        try:
            logger.info(f"ğŸ¨ ì‚¬ìš©ì {user_id}ì˜ ì´ë¯¸ì§€ ìƒì„± ì‹œì‘")
            logger.info(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {input_text}")

            # 1. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ë¡œë“œ
            user_profile = self.get_user_profile(user_id)

            # 2. ê°ì • ë¶„ì„
            emotion = self.emotion_mapper.extract_emotion_from_text(input_text)
            logger.info(
                f"ğŸ˜Š ê°ì • ë¶„ì„: V={emotion.valence:.3f}, A={emotion.arousal:.3f}, D={emotion.dominance:.3f}"
            )

            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            emotion_modifiers = self.emotion_mapper.emotion_to_prompt_modifiers(emotion)
            personal_modifiers = user_profile.get_personalized_style_modifiers()

            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not base_prompt:
                base_prompt = "digital art, beautiful scene"

            # ì¼ê´€ëœ í…Œë§ˆ ì ìš©
            style_prompt = "watercolor, pastel tone"
            
            final_prompt = f"{base_prompt}, {style_prompt}, {emotion_modifiers}, {personal_modifiers}"
            final_prompt += ", high quality, detailed, masterpiece"

            logger.info(f"ğŸ¯ ìµœì¢… í”„ë¡¬í”„íŠ¸: {final_prompt}")

            # 4. ì´ë¯¸ì§€ ìƒì„±
            if self.pipeline:
                # ë¶€ì • í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                negative_prompt = "photorealistic, 3d render, harsh lighting, ugly, deformed, noisy, blurry"
                
                # SD íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
                with torch.autocast(
                    self.device.type if self.device.type != "mps" else "cpu"
                ):
                    result = self.pipeline(
                        prompt=final_prompt,
                        negative_prompt=negative_prompt,
                        num_inference_steps=num_inference_steps,
                        guidance_scale=guidance_scale,
                        width=width,
                        height=height,
                        output_type="pil",
                    )

                generated_image = result.images[0]
                logger.info("âœ… SD íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
            else:
                # í´ë°±: ê°„ë‹¨í•œ ì´ë¯¸ì§€ ìƒì„±
                generated_image = self._generate_fallback_image(emotion, width, height)
                logger.info("âš ï¸ í´ë°± ì´ë¯¸ì§€ ìƒì„±ê¸° ì‚¬ìš©")

            # 5. ì´ë¯¸ì§€ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            image_filename = f"{user_id}_{timestamp}.png"
            image_path = self.output_dir / image_filename
            generated_image.save(image_path)

            # 6. ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ë¡
            emotion_id = user_profile.add_emotion_record(
                input_text=input_text,
                emotion=emotion,
                generated_prompt=final_prompt,
                image_path=str(image_path),
            )

            # 7. ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "emotion_id": emotion_id,
                "user_id": user_id,
                "input_text": input_text,
                "emotion": emotion.to_dict(),
                "final_prompt": final_prompt,
                "image_path": str(image_path),
                "image_filename": image_filename,
                "generation_params": {
                    "num_inference_steps": num_inference_steps,
                    "guidance_scale": guidance_scale,
                    "width": width,
                    "height": height,
                },
                "timestamp": timestamp,
                "device": str(self.device),
            }

            logger.info(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {image_path}")
            return {"success": True, "image": generated_image, "metadata": metadata}

        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metadata": {"user_id": user_id, "input_text": input_text},
            }

    def _generate_fallback_image(
        self, emotion: EmotionEmbedding, width: int = 512, height: int = 512
    ) -> Image.Image:
        """í´ë°± ì´ë¯¸ì§€ ìƒì„± (SD íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ì‹œ)"""

        # ê°ì • ê¸°ë°˜ ìƒ‰ìƒ ìƒì„±
        if emotion.valence > 0.3:
            # ê¸ì •ì  ê°ì • - ë”°ëœ»í•œ ìƒ‰ìƒ
            base_color = [0.9, 0.8, 0.6]  # ë”°ëœ»í•œ ë…¸ë€ìƒ‰
        elif emotion.valence < -0.3:
            # ë¶€ì •ì  ê°ì • - ì°¨ê°€ìš´ ìƒ‰ìƒ
            base_color = [0.6, 0.7, 0.9]  # ì°¨ê°€ìš´ íŒŒë€ìƒ‰
        else:
            # ì¤‘ì„± ê°ì • - ì¤‘ê°„ ìƒ‰ìƒ
            base_color = [0.7, 0.7, 0.8]  # íšŒìƒ‰ë¹›

        # ê°ì„±ë„ ê¸°ë°˜ ê°•ë„ ì¡°ì •
        intensity = 0.5 + abs(emotion.arousal) * 0.5
        base_color = [c * intensity for c in base_color]

        # ê·¸ë¼ë°ì´ì…˜ ì´ë¯¸ì§€ ìƒì„±
        image_array = np.zeros((height, width, 3))

        for i in range(height):
            for j in range(width):
                # ì¤‘ì‹¬ì—ì„œì˜ ê±°ë¦¬ ê¸°ë°˜ ê·¸ë¼ë°ì´ì…˜
                center_x, center_y = width // 2, height // 2
                distance = np.sqrt((j - center_x) ** 2 + (i - center_y) ** 2)
                max_distance = np.sqrt(center_x**2 + center_y**2)

                # ê°ì • ê¸°ë°˜ ê·¸ë¼ë°ì´ì…˜ íŒ¨í„´
                if emotion.dominance > 0:
                    # ì§€ë°°ì  ê°ì • - ì¤‘ì‹¬ì—ì„œ ë°”ê¹¥ìœ¼ë¡œ
                    factor = 1.0 - (distance / max_distance) * 0.5
                else:
                    # ìˆ˜ë™ì  ê°ì • - ë°”ê¹¥ì—ì„œ ì¤‘ì‹¬ìœ¼ë¡œ
                    factor = 0.5 + (distance / max_distance) * 0.5

                image_array[i, j] = [c * factor for c in base_color]

        # numpy ë°°ì—´ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        image_array = np.clip(image_array * 255, 0, 255).astype(np.uint8)
        return Image.fromarray(image_array)

    def process_feedback(
        self,
        user_id: str,
        emotion_id: int,
        feedback_score: float,
        feedback_type: str = "rating",
        comments: str = None,
        enable_training: bool = True,
        interaction_metadata: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”¼ë“œë°± ì²˜ë¦¬ ë° ê°œì¸í™” í•™ìŠµ (ìŠ¤ë§ˆíŠ¸ í”¼ë“œë°± ì ìš©)"""

        try:
            logger.info(f"ğŸ“ ì‚¬ìš©ì {user_id} í”¼ë“œë°± ì²˜ë¦¬: ì ìˆ˜ {feedback_score}")

            # 1. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ë¡œë“œ
            user_profile = self.get_user_profile(user_id)
            
            # 2. í•´ë‹¹ ê°ì • ê¸°ë¡ ì°¾ê¸° (ìŠ¤ë§ˆíŠ¸ í”¼ë“œë°±ì„ ìœ„í•´)
            target_emotion_record = None
            for record in user_profile.emotion_history:
                if record.get("id") == emotion_id:
                    target_emotion_record = record
                    break
            
            # 3. ìŠ¤ë§ˆíŠ¸ í”¼ë“œë°± í–¥ìƒ ì²˜ë¦¬
            enhanced_feedback = None
            if target_emotion_record:
                try:
                    # ìƒí˜¸ì‘ìš© ë©”íƒ€ë°ì´í„° ê¸°ë³¸ê°’ ì„¤ì •
                    if interaction_metadata is None:
                        interaction_metadata = {
                            "viewing_time": 5.0,  # ê¸°ë³¸ 5ì´ˆ
                            "response_time": 10.0,
                            "hesitation_count": 0
                        }
                    
                    # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    image_metadata = target_emotion_record.get("image_metadata", {
                        "brightness": 0.5,
                        "saturation": 0.5,
                        "contrast": 0.5
                    })
                    
                    # ìŠ¤ë§ˆíŠ¸ í”¼ë“œë°± í–¥ìƒ
                    enhanced_feedback = self.feedback_enhancer.enhance_simple_feedback(
                        simple_score=feedback_score,
                        user_emotion=target_emotion_record["emotion"],
                        image_metadata=image_metadata,
                        user_history=user_profile.emotion_history,
                        interaction_time=interaction_metadata.get("viewing_time", 5.0)
                    )
                    
                    logger.info(f"âœ… ìŠ¤ë§ˆíŠ¸ í”¼ë“œë°± í–¥ìƒ ì™„ë£Œ: {len(enhanced_feedback.get('inferred_aspects', {}))}ê°œ ì„ í˜¸ë„ ì¶”ë¡ ")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ìŠ¤ë§ˆíŠ¸ í”¼ë“œë°± í–¥ìƒ ì‹¤íŒ¨: {e}, ê¸°ë³¸ í”¼ë“œë°± ì‚¬ìš©")
                    enhanced_feedback = None

            # 4. ê¸°ë³¸ í”¼ë“œë°± ì €ì¥ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
            user_profile.add_feedback(
                emotion_id=emotion_id,
                feedback_score=feedback_score,
                feedback_type=feedback_type,
                comments=comments,
            )
            
            # 5. í–¥ìƒëœ í”¼ë“œë°± ì •ë³´ë¥¼ ì ì‘í˜• ì‹œìŠ¤í…œì— ì¶”ê°€ë¡œ ì œê³µ
            if enhanced_feedback and hasattr(user_profile, 'adaptive_system'):
                try:
                    self._apply_enhanced_feedback_to_adaptive_system(
                        user_profile, target_emotion_record, enhanced_feedback
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ í–¥ìƒëœ í”¼ë“œë°± ì ìš© ì‹¤íŒ¨: {e}")

            # 3. ê°•í™”í•™ìŠµ ìˆ˜í–‰ (ì˜µì…˜)
            training_result = None
            if (
                enable_training and self.trainer and feedback_score != 3.0
            ):  # ì¤‘ì„± í”¼ë“œë°± ì œì™¸

                # í•´ë‹¹ ê°ì • ê¸°ë¡ ì°¾ê¸°
                emotion_record = None
                for record in user_profile.emotion_history:
                    if record.get("id") == emotion_id:
                        emotion_record = record
                        break

                if emotion_record:
                    logger.info("ğŸ¤– ê°œì¸í™” í•™ìŠµ ì‹œì‘...")
                    training_result = self.trainer.train_step(
                        prompt=emotion_record["generated_prompt"],
                        target_emotion=emotion_record["emotion"],
                        user_profile=user_profile,
                        num_inference_steps=8,  # ë¹ ë¥¸ í•™ìŠµ
                    )
                    logger.info(
                        f"âœ… í•™ìŠµ ì™„ë£Œ: ë³´ìƒ {training_result.get('total_reward', 0):.3f}"
                    )

            # 4. LoRA ì–´ëŒ‘í„° ì €ì¥ (ì£¼ê¸°ì )
            if len(user_profile.feedback_history) % 5 == 0:  # 5ë²ˆì§¸ í”¼ë“œë°±ë§ˆë‹¤ ì €ì¥
                self._save_user_lora_if_needed(user_id, user_profile)

            # 5. ì¹˜ë£Œ ì¸ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸
            insights = user_profile.get_therapeutic_insights()

            result = {
                "success": True,
                "feedback_recorded": True,
                "training_performed": training_result is not None,
                "training_result": training_result,
                "therapeutic_insights": insights,
                "total_interactions": len(user_profile.emotion_history),
                "total_feedbacks": len(user_profile.feedback_history),
            }

            # 7. ê²°ê³¼ì— í–¥ìƒëœ í”¼ë“œë°± ì •ë³´ í¬í•¨
            result = {
                "success": True,
                "feedback_recorded": True,
                "training_performed": training_result is not None,
                "training_result": training_result,
                "therapeutic_insights": insights,
                "total_interactions": len(user_profile.emotion_history),
                "total_feedbacks": len(user_profile.feedback_history),
            }
            
            # í–¥ìƒëœ í”¼ë“œë°± ì •ë³´ ì¶”ê°€
            if enhanced_feedback:
                result["enhanced_feedback"] = {
                    "emotion_alignment": enhanced_feedback["enhanced_insights"].get("emotion_alignment"),
                    "visual_preferences": enhanced_feedback["enhanced_insights"].get("visual_preferences"),
                    "engagement_analysis": enhanced_feedback["enhanced_insights"].get("engagement"),
                    "inferred_preferences": enhanced_feedback["inferred_aspects"]
                }
                logger.info("âœ… í–¥ìƒëœ í”¼ë“œë°± ì •ë³´ í¬í•¨")

            logger.info("âœ… í”¼ë“œë°± ì²˜ë¦¬ ì™„ë£Œ")
            return result

        except Exception as e:
            logger.error(f"âŒ í”¼ë“œë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "feedback_recorded": False,
                "training_performed": False,
            }
    
    def _apply_enhanced_feedback_to_adaptive_system(
        self,
        user_profile: UserEmotionProfile,
        emotion_record: Dict[str, Any],
        enhanced_feedback: Dict[str, Any]
    ):
        """í–¥ìƒëœ í”¼ë“œë°± ì •ë³´ë¥¼ ì ì‘í˜• ì‹œìŠ¤í…œì— ì ìš©"""
        
        try:
            # ì¶”ë¡ ëœ ì„ í˜¸ë„ ì •ë³´ ì¶”ì¶œ
            inferred_aspects = enhanced_feedback.get("inferred_aspects", {})
            enhanced_insights = enhanced_feedback.get("enhanced_insights", {})
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            enhanced_metadata = emotion_record.get("image_metadata", {}).copy()
            
            # ì¶”ë¡ ëœ ì„ í˜¸ë„ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
            if "preferred_brightness" in inferred_aspects:
                brightness_map = {"bright": 0.8, "dim": 0.3, "normal": 0.5}
                enhanced_metadata["inferred_brightness_pref"] = brightness_map.get(
                    inferred_aspects["preferred_brightness"], 0.5
                )
            
            if "preferred_saturation" in inferred_aspects:
                saturation_map = {"vibrant": 0.8, "muted": 0.3, "normal": 0.5}
                enhanced_metadata["inferred_saturation_pref"] = saturation_map.get(
                    inferred_aspects["preferred_saturation"], 0.5
                )
            
            # ì°¸ì—¬ë„ ì •ë³´ ì¶”ê°€
            engagement = enhanced_insights.get("engagement", {})
            if engagement:
                enhanced_metadata["engagement_level"] = {
                    "high": 0.8, "appropriate": 0.6, "low": 0.3
                }.get(engagement.get("engagement_level"), 0.5)
                
                enhanced_metadata["viewing_time_normalized"] = min(
                    engagement.get("viewing_time", 5.0) / 20.0, 1.0
                )
            
            # ê°ì •-ì ìˆ˜ ì¼ì¹˜ë„ ì •ë³´
            emotion_alignment = enhanced_insights.get("emotion_alignment", {})
            if emotion_alignment:
                enhanced_metadata["emotion_score_gap"] = emotion_alignment.get("gap", 0.0)
                enhanced_metadata["therapeutic_effect"] = 1.0 if emotion_alignment.get("gap", 0) > 0.5 else 0.5
            
            # ì ì‘í˜• ì‹œìŠ¤í…œì— í’ë¶€í•œ ì •ë³´ë¡œ í”¼ë“œë°± ì¬ì „ì†¡
            if hasattr(user_profile, 'adaptive_system') and user_profile.adaptive_system:
                user_profile.adaptive_system.add_feedback(
                    user_id=user_profile.user_id,
                    emotion=emotion_record["emotion"],
                    image_metadata=enhanced_metadata,  # í–¥ìƒëœ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
                    prompt=emotion_record.get("generated_prompt", ""),
                    feedback_score=enhanced_feedback["original_score"]
                )
                
                logger.info("âœ… í–¥ìƒëœ í”¼ë“œë°± ì •ë³´ë¡œ ì ì‘í˜• ì‹œìŠ¤í…œ ì¬í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ í–¥ìƒëœ í”¼ë“œë°± ì ìš© ì‹¤íŒ¨: {e}")
            raise

    def _save_user_lora_if_needed(self, user_id: str, user_profile: UserEmotionProfile):
        """í•„ìš”ì‹œ ì‚¬ìš©ì LoRA ì–´ëŒ‘í„° ì €ì¥"""
        try:
            if self.pipeline and hasattr(self.pipeline, "unet"):
                # í˜„ì¬ ëª¨ë¸ ìƒíƒœë¥¼ LoRAë¡œ ì €ì¥
                model_state = {
                    "unet_state_dict": self.pipeline.unet.state_dict(),
                    "user_preferences": user_profile.preference_weights,
                    "training_metadata": user_profile.learning_metadata,
                }

                self.lora_manager.save_user_lora(user_id, model_state)
                logger.info(f"ğŸ’¾ ì‚¬ìš©ì {user_id} LoRA ì–´ëŒ‘í„° ì €ì¥")
        except Exception as e:
            logger.warning(f"âš ï¸ LoRA ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_user_insights(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì¹˜ë£Œ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        user_profile = self.get_user_profile(user_id)
        return user_profile.get_therapeutic_insights()

    def get_emotion_history(self, user_id: str, limit: int = 10) -> List[Dict]:
        """ì‚¬ìš©ì ê°ì • íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        user_profile = self.get_user_profile(user_id)
        return user_profile.emotion_history[-limit:]

    def cleanup_old_images(self, days_old: int = 30):
        """ì˜¤ë˜ëœ ì´ë¯¸ì§€ íŒŒì¼ ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_old)
            cleaned_count = 0

            for image_file in self.output_dir.glob("*.png"):
                if image_file.stat().st_mtime < cutoff_date.timestamp():
                    image_file.unlink()
                    cleaned_count += 1

            logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ì´ë¯¸ì§€ {cleaned_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
            return cleaned_count

        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0