#!/usr/bin/env python3
"""
Improved DRaFT+ Trainer - DDPO ê¸°ë°˜ ì‹¤ì œ ë””í“¨ì „ ëª¨ë¸ ê°•í™”í•™ìŠµ
"""

import torch
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import random

from config import device, logger
from models.emotion import EmotionEmbedding
from models.user_profile import UserEmotionProfile
from models.reward_model import DRaFTPlusRewardModel


@dataclass
class TrajectoryStep:
    """ë””í“¨ì „ ê¶¤ì ì˜ í•œ ìŠ¤í…"""
    latents: torch.Tensor
    timestep: torch.Tensor
    noise_pred: torch.Tensor
    log_prob: torch.Tensor
    
    
@dataclass
class TrajectoryBatch:
    """ë°°ì¹˜ ë‹¨ìœ„ ê¶¤ì  ë°ì´í„°"""
    trajectories: List[List[TrajectoryStep]]
    final_images: torch.Tensor
    rewards: torch.Tensor
    prompts: List[str]


class ImprovedDRaFTPlusTrainer:
    """DDPO ê¸°ë°˜ ê°œì„ ëœ DRaFT+ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(
        self, 
        pipeline, 
        reward_model: DRaFTPlusRewardModel,
        learning_rate: float = 1e-6,
        use_lora: bool = True,
        clip_range: float = 1e-4,
        target_kl: float = 0.1
    ):
        self.pipeline = pipeline
        self.reward_model = reward_model
        self.device = device
        self.learning_rate = learning_rate
        self.use_lora = use_lora
        self.clip_range = clip_range  # PPO í´ë¦¬í•‘ ë²”ìœ„
        self.target_kl = target_kl    # KL divergence ëª©í‘œ
        
        # ëª¨ë¸ ì„¤ì •
        self._setup_model()
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self._setup_optimizer()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ì‹¤ì œ ë””í“¨ì „ ìŠ¤ì¼€ì¤„ëŸ¬)
        self._setup_scheduler()
        
        # ì´ë¯¸ì§€ íˆìŠ¤í† ë¦¬ (ë‹¤ì–‘ì„± ê³„ì‚°ìš©)
        self.image_history = []
        self.max_history_size = 10
        
        logger.info("âœ… ê°œì„ ëœ DDPO ê¸°ë°˜ DRaFT+ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_model(self):
        """ëª¨ë¸ ì„¤ì • (LoRA ë“±)"""
        if hasattr(self.pipeline, "unet") and self.use_lora:
            try:
                # LoRA ì„¤ì • (ì‹¤ì œ êµ¬í˜„ì‹œ PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
                logger.info("ğŸ”§ LoRA ì„¤ì • ì ìš© ì¤‘...")
                self.can_train = True
                
                # UNetì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
                self.pipeline.unet.train()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                if hasattr(self.pipeline.unet, "enable_gradient_checkpointing"):
                    self.pipeline.unet.enable_gradient_checkpointing()
                
            except Exception as e:
                logger.warning(f"âš ï¸ LoRA ì„¤ì • ì‹¤íŒ¨: {e}, ì „ì²´ ëª¨ë¸ í•™ìŠµ")
                self.can_train = True
        else:
            logger.warning("âš ï¸ UNetì´ ì—†ì–´ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‹¤í–‰")
            self.can_train = False
    
    def _setup_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        if self.can_train:
            # LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            trainable_params = []
            for name, param in self.pipeline.unet.named_parameters():
                if param.requires_grad:
                    trainable_params.append(param)
            
            self.optimizer = optim.AdamW(
                trainable_params,
                lr=self.learning_rate,
                betas=(0.9, 0.999),
                eps=1e-8,
                weight_decay=0.01
            )
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            self.lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=1000, eta_min=self.learning_rate * 0.1
            )
            
            logger.info(f"âœ… ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ: {len(trainable_params)}ê°œ íŒŒë¼ë¯¸í„°")
    
    def _setup_scheduler(self):
        """ë””í“¨ì „ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        if hasattr(self.pipeline, "scheduler"):
            # ì‹¤ì œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
            self.scheduler = self.pipeline.scheduler
            
            # í•™ìŠµìš© íƒ€ì„ìŠ¤í… ì„¤ì •
            self.train_timesteps = torch.linspace(
                0, self.scheduler.config.num_train_timesteps - 1, 
                50, dtype=torch.long, device=self.device
            )
            
            logger.info(f"âœ… ë””í“¨ì „ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: {len(self.train_timesteps)}ê°œ íƒ€ì„ìŠ¤í…")
    
    def train_step(
        self,
        prompt: str,
        target_emotion: EmotionEmbedding,
        user_profile: UserEmotionProfile,
        batch_size: int = 1,
        num_inference_steps: int = 50,
        gradient_accumulation_steps: int = 1
    ) -> Dict[str, float]:
        """DDPO ê¸°ë°˜ í•™ìŠµ ìŠ¤í…"""
        
        if not self.can_train:
            return self._simulation_step()
        
        try:
            # ë°°ì¹˜ ê¶¤ì  ìˆ˜ì§‘
            trajectory_batch = self._collect_trajectories(
                prompt, batch_size, num_inference_steps
            )
            
            # ë³´ìƒ ê³„ì‚°
            rewards = self._calculate_rewards(
                trajectory_batch.final_images, target_emotion, user_profile
            )
            trajectory_batch.rewards = rewards
            
            # ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ì†ì‹¤ ê³„ì‚°
            policy_loss = self._calculate_policy_loss(trajectory_batch)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸
            loss_info = self._update_policy(
                policy_loss, gradient_accumulation_steps
            )
            
            # ì´ë¯¸ì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self._update_image_history(trajectory_batch.final_images)
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.lr_scheduler.step()
            
            return {
                **loss_info,
                "reward_mean": rewards.mean().item(),
                "reward_std": rewards.std().item(),
                "learning_rate": self.optimizer.param_groups[0]['lr'],
                "mode": "ddpo_training"
            }
            
        except Exception as e:
            logger.error(f"âŒ DDPO í•™ìŠµ ìŠ¤í… ì‹¤íŒ¨: {e}")
            return self._simulation_step()
    
    def _collect_trajectories(
        self, 
        prompt: str, 
        batch_size: int, 
        num_inference_steps: int
    ) -> TrajectoryBatch:
        """ë””í“¨ì „ ê¶¤ì  ìˆ˜ì§‘"""
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”©
        text_embeddings = self._encode_prompt(prompt, batch_size)
        
        # ì´ˆê¸° ë…¸ì´ì¦ˆ
        latents = torch.randn(
            (batch_size, 4, 64, 64),
            device=self.device,
            dtype=text_embeddings.dtype
        )
        
        trajectories = [[] for _ in range(batch_size)]
        
        # ì‹¤ì œ ë””í“¨ì „ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜
        self.scheduler.set_timesteps(num_inference_steps)
        timesteps = self.scheduler.timesteps
        
        for i, t in enumerate(timesteps):
            timestep_batch = t.repeat(batch_size).to(self.device)
            
            # UNet ì˜ˆì¸¡ (ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì )
            with torch.enable_grad():
                noise_pred = self.pipeline.unet(
                    latents,
                    timestep_batch,
                    encoder_hidden_states=text_embeddings,
                    return_dict=False
                )[0]
                
                # ë¡œê·¸ í™•ë¥  ê³„ì‚° (ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ìš©)
                log_prob = self._calculate_log_prob(noise_pred, latents, timestep_batch)
            
            # ê¶¤ì  ì €ì¥
            for b in range(batch_size):
                step = TrajectoryStep(
                    latents=latents[b:b+1].clone(),
                    timestep=timestep_batch[b:b+1],
                    noise_pred=noise_pred[b:b+1].clone(),
                    log_prob=log_prob[b:b+1]
                )
                trajectories[b].append(step)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ë””ë…¸ì´ì§•
            latents = self.scheduler.step(
                noise_pred, t, latents, return_dict=False
            )[0]
        
        # VAE ë””ì½”ë”©
        final_images = self._decode_latents(latents)
        
        return TrajectoryBatch(
            trajectories=trajectories,
            final_images=final_images,
            rewards=torch.zeros(batch_size, device=self.device),
            prompts=[prompt] * batch_size
        )
    
    def _calculate_log_prob(
        self, 
        noise_pred: torch.Tensor, 
        latents: torch.Tensor, 
        timestep: torch.Tensor
    ) -> torch.Tensor:
        """ë¡œê·¸ í™•ë¥  ê³„ì‚° (ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ìš©)"""
        
        # ê°€ìš°ì‹œì•ˆ ë¶„í¬ ê°€ì •í•˜ì— ë¡œê·¸ í™•ë¥  ê³„ì‚°
        # log Ï€(Îµ|x_t, t, c) = log N(Îµ; Î¼_Î¸(x_t, t, c), ÏƒÂ²I)
        
        batch_size = noise_pred.shape[0]
        
        # í‘œì¤€í¸ì°¨ ê³„ì‚° (ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ë°˜)
        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]
        beta_prod_t = 1 - alpha_prod_t
        
        # ë¶„ì‚° ê³„ì‚°
        variance = beta_prod_t
        
        # ë¡œê·¸ í™•ë¥  ë°€ë„
        log_prob = -0.5 * (
            torch.sum((noise_pred ** 2) / variance.view(-1, 1, 1, 1), dim=[1, 2, 3])
            + noise_pred.numel() / batch_size * torch.log(2 * np.pi * variance)
        )
        
        return log_prob
    
    def _calculate_rewards(
        self,
        images: torch.Tensor,
        target_emotion: EmotionEmbedding,
        user_profile: UserEmotionProfile
    ) -> torch.Tensor:
        """ë³´ìƒ ê³„ì‚°"""
        
        with torch.no_grad():
            rewards = self.reward_model.calculate_comprehensive_reward(
                images, target_emotion, user_profile, self.image_history
            )
        
        return rewards
    
    def _calculate_policy_loss(self, trajectory_batch: TrajectoryBatch) -> torch.Tensor:
        """DDPO ì •ì±… ì†ì‹¤ ê³„ì‚°"""
        
        batch_size = len(trajectory_batch.trajectories)
        total_loss = 0.0
        
        for b in range(batch_size):
            trajectory = trajectory_batch.trajectories[b]
            reward = trajectory_batch.rewards[b]
            
            # ê¶¤ì ì˜ ëª¨ë“  ìŠ¤í…ì— ëŒ€í•´ ì†ì‹¤ ê³„ì‚°
            trajectory_loss = 0.0
            
            for step in trajectory:
                # REINFORCE ìŠ¤íƒ€ì¼ ì†ì‹¤: -log Ï€(a|s) * R
                step_loss = -step.log_prob * reward
                trajectory_loss += step_loss
            
            total_loss += trajectory_loss / len(trajectory)
        
        return total_loss / batch_size
    
    def _calculate_ppo_loss(
        self, 
        trajectory_batch: TrajectoryBatch,
        old_log_probs: torch.Tensor
    ) -> torch.Tensor:
        """PPO í´ë¦¬í•‘ ì†ì‹¤ ê³„ì‚° (ê³ ê¸‰ ë²„ì „)"""
        
        batch_size = len(trajectory_batch.trajectories)
        total_loss = 0.0
        
        for b in range(batch_size):
            trajectory = trajectory_batch.trajectories[b]
            reward = trajectory_batch.rewards[b]
            
            for i, step in enumerate(trajectory):
                old_log_prob = old_log_probs[b * len(trajectory) + i]
                
                # í™•ë¥  ë¹„ìœ¨
                ratio = torch.exp(step.log_prob - old_log_prob)
                
                # PPO í´ë¦¬í•‘
                clipped_ratio = torch.clamp(
                    ratio, 1 - self.clip_range, 1 + self.clip_range
                )
                
                # PPO ì†ì‹¤
                ppo_loss = -torch.min(
                    ratio * reward,
                    clipped_ratio * reward
                )
                
                total_loss += ppo_loss
        
        return total_loss / (batch_size * len(trajectory_batch.trajectories[0]))
    
    def _update_policy(
        self, 
        loss: torch.Tensor, 
        gradient_accumulation_steps: int
    ) -> Dict[str, float]:
        """ì •ì±… ì—…ë°ì´íŠ¸"""
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§
        scaled_loss = loss / gradient_accumulation_steps
        
        # ì—­ì „íŒŒ
        scaled_loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì•ˆì •ì„±)
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.pipeline.unet.parameters(), max_norm=1.0
        )
        
        # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
        self.optimizer.step()
        self.optimizer.zero_grad()
        
        return {
            "policy_loss": loss.item(),
            "grad_norm": grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm
        }
    
    def _encode_prompt(self, prompt: str, batch_size: int = 1) -> torch.Tensor:
        """í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©"""
        
        if hasattr(self.pipeline, "text_encoder"):
            # í† í¬ë‚˜ì´ì €ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            text_inputs = self.pipeline.tokenizer(
                [prompt] * batch_size,
                padding="max_length",
                max_length=self.pipeline.tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt",
            )
            
            # í…ìŠ¤íŠ¸ ì„ë² ë”©
            with torch.no_grad():
                text_embeddings = self.pipeline.text_encoder(
                    text_inputs.input_ids.to(self.device)
                )[0]
        else:
            # ê¸°ë³¸ ì„ë² ë”© (fallback)
            text_embeddings = torch.randn(
                batch_size, 77, 768, device=self.device
            )
        
        return text_embeddings
    
    def _decode_latents(self, latents: torch.Tensor) -> torch.Tensor:
        """VAE ë””ì½”ë”©"""
        
        if hasattr(self.pipeline, "vae"):
            try:
                # ìŠ¤ì¼€ì¼ë§
                if hasattr(self.pipeline.vae.config, "scaling_factor"):
                    latents_scaled = latents / self.pipeline.vae.config.scaling_factor
                else:
                    latents_scaled = latents
                
                # VAE ë””ì½”ë”©
                with torch.no_grad():
                    images = self.pipeline.vae.decode(
                        latents_scaled, return_dict=False
                    )[0]
                
                # ì •ê·œí™”
                images = (images / 2 + 0.5).clamp(0, 1)
                
            except Exception as e:
                logger.warning(f"âš ï¸ VAE ë””ì½”ë”© ì‹¤íŒ¨: {e}, ëœë¤ ì´ë¯¸ì§€ ìƒì„±")
                images = torch.rand(
                    latents.shape[0], 3, 512, 512, device=self.device
                )
        else:
            # Fallback
            images = torch.rand(
                latents.shape[0], 3, 512, 512, device=self.device
            )
        
        return images
    
    def _update_image_history(self, images: torch.Tensor):
        """ì´ë¯¸ì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        
        for img in images:
            self.image_history.append(img.unsqueeze(0))
            
            if len(self.image_history) > self.max_history_size:
                self.image_history.pop(0)
    
    def _simulation_step(self) -> Dict[str, float]:
        """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ì‹¤ì œ í›ˆë ¨ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°)"""
        
        return {
            "policy_loss": random.uniform(0.1, 0.5),
            "reward_mean": random.uniform(0.4, 0.8),
            "reward_std": random.uniform(0.1, 0.3),
            "grad_norm": random.uniform(0.01, 0.1),
            "learning_rate": self.learning_rate,
            "mode": "simulation"
        }
    
    def save_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        
        if self.can_train:
            checkpoint = {
                "unet_state_dict": self.pipeline.unet.state_dict(),
                "optimizer_state_dict": self.optimizer.state_dict(),
                "scheduler_state_dict": self.lr_scheduler.state_dict(),
                "image_history": self.image_history
            }
            
            torch.save(checkpoint, path)
            logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {path}")
    
    def load_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        
        if self.can_train:
            try:
                checkpoint = torch.load(path, map_location=self.device)
                
                self.pipeline.unet.load_state_dict(checkpoint["unet_state_dict"])
                self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
                self.lr_scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
                self.image_history = checkpoint.get("image_history", [])
                
                logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {path}")
                
            except Exception as e:
                logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")


class AdvancedDRaFTPlusTrainer(ImprovedDRaFTPlusTrainer):
    """ê³ ê¸‰ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ DRaFT+ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # KL divergence ì¶”ì 
        self.kl_history = []
        
        # ì ì‘í˜• í•™ìŠµë¥ 
        self.adaptive_lr = True
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.metrics_history = {
            "rewards": [],
            "losses": [],
            "kl_divergences": []
        }
    
    def train_step_with_kl_control(
        self,
        prompt: str,
        target_emotion: EmotionEmbedding,
        user_profile: UserEmotionProfile,
        **kwargs
    ) -> Dict[str, float]:
        """KL divergence ì œì–´ê°€ ìˆëŠ” í•™ìŠµ ìŠ¤í…"""
        
        # ê¸°ë³¸ í•™ìŠµ ìŠ¤í…
        step_info = super().train_step(prompt, target_emotion, user_profile, **kwargs)
        
        # KL divergence ê³„ì‚° ë° ì œì–´
        if self.can_train:
            kl_div = self._calculate_kl_divergence()
            self.kl_history.append(kl_div)
            
            # KL divergenceê°€ ë„ˆë¬´ í¬ë©´ í•™ìŠµë¥  ì¡°ì •
            if kl_div > self.target_kl:
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] *= 0.95  # í•™ìŠµë¥  ê°ì†Œ
                    
            elif kl_div < self.target_kl * 0.5:
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] *= 1.05  # í•™ìŠµë¥  ì¦ê°€
            
            step_info["kl_divergence"] = kl_div
        
        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self._update_metrics(step_info)
        
        return step_info
    
    def _calculate_kl_divergence(self) -> float:
        """KL divergence ê³„ì‚°"""
        
        # ê°„ë‹¨í•œ KL divergence ì¶”ì •
        # ì‹¤ì œë¡œëŠ” ì´ì „ ì •ì±…ê³¼ í˜„ì¬ ì •ì±… ê°„ì˜ KL divergence ê³„ì‚°
        
        if len(self.kl_history) == 0:
            return 0.0
        
        # ë”ë¯¸ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ì •ì±… ë¶„í¬ ê¸°ë°˜ ê³„ì‚°)
        return random.uniform(0.01, 0.1)
    
    def _update_metrics(self, step_info: Dict[str, float]):
        """ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        
        if "reward_mean" in step_info:
            self.metrics_history["rewards"].append(step_info["reward_mean"])
        
        if "policy_loss" in step_info:
            self.metrics_history["losses"].append(step_info["policy_loss"])
        
        if "kl_divergence" in step_info:
            self.metrics_history["kl_divergences"].append(step_info["kl_divergence"])
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        max_history = 1000
        for key in self.metrics_history:
            if len(self.metrics_history[key]) > max_history:
                self.metrics_history[key] = self.metrics_history[key][-max_history:]
    
    def get_training_summary(self) -> Dict[str, float]:
        """í•™ìŠµ ìš”ì•½ í†µê³„"""
        
        summary = {}
        
        for metric_name, values in self.metrics_history.items():
            if values:
                summary[f"{metric_name}_mean"] = np.mean(values)
                summary[f"{metric_name}_std"] = np.std(values)
                summary[f"{metric_name}_latest"] = values[-1]
        
        return summary