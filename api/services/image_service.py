# api/services/image_service.py

import httpx
import base64
import asyncio
from typing import Dict, Any, Optional, List
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
import logging

from api.config import settings

logger = logging.getLogger(__name__)


class ImageGenerationStrategy(ABC):
    """ì´ë¯¸ì§€ ìƒì„± ì „ëµ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ìƒì„± ì¶”ìƒ ë©”ì„œë“œ"""
        pass

    @abstractmethod
    def get_status(self) -> Dict[str, Any]:
        """ë°±ì—”ë“œ ìƒíƒœ í™•ì¸"""
        pass


class LocalGPUStrategy(ImageGenerationStrategy):
    """ë¡œì»¬ GPUë¥¼ ì‚¬ìš©í•œ Stable Diffusion"""

    def __init__(self):
        self.generator = None
        self.model_path = settings.local_model_path
        self.is_initialized = False

        # ì´ˆê¸°í™”ëŠ” ì²« ì‚¬ìš© ì‹œ ì§€ì—° ë¡œë”©
        logger.info(f"ğŸ–¼ï¸  LocalGPU ì „ëµ ì´ˆê¸°í™” (ëª¨ë¸: {self.model_path})")

    def _initialize_generator(self):
        """ì§€ì—° ì´ˆê¸°í™”: ì‹¤ì œ ì‚¬ìš© ì‹œì ì— ëª¨ë¸ ë¡œë“œ"""
        if self.is_initialized:
            return

        try:
            # ê¸°ì¡´ ImageGenerator í´ë˜ìŠ¤ í™œìš©
            from src.services.image_generator import ImageGenerator

            self.generator = ImageGenerator(self.model_path)
            self.is_initialized = True
            logger.info("âœ… ë¡œì»¬ GPU ì´ë¯¸ì§€ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ GPU ì´ë¯¸ì§€ ìƒì„±ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ë¡œì»¬ GPUë¡œ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # ì§€ì—° ì´ˆê¸°í™”
            if not self.is_initialized:
                self._initialize_generator()

            # ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•´ executor ì‚¬ìš©
            loop = asyncio.get_event_loop()

            # ê¸°ë³¸ ì„¤ì •
            generation_params = {
                "width": kwargs.get("width", 512),
                "height": kwargs.get("height", 512),
                "num_inference_steps": kwargs.get("num_inference_steps", 20),
                "guidance_scale": kwargs.get("guidance_scale", 7.5),
                "seed": kwargs.get("seed", None),
            }

            # ë¸”ë¡œí‚¹ í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            result = await loop.run_in_executor(
                None,
                self.generator.generate_image,
                prompt,
                generation_params.get("width"),
                generation_params.get("height"),
                generation_params.get("num_inference_steps"),
                generation_params.get("guidance_scale"),
                generation_params.get("seed"),
            )

            if result["success"]:
                logger.info(f"âœ… ë¡œì»¬ GPU ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {prompt[:50]}...")

                # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë°˜í™˜
                if result.get("image"):
                    import io

                    img_bytes = io.BytesIO()
                    result["image"].save(img_bytes, format="PNG")
                    img_bytes.seek(0)

                    image_b64 = base64.b64encode(img_bytes.getvalue()).decode("utf-8")

                    return {
                        "success": True,
                        "image_b64": image_b64,
                        "image_url": None,  # ë¡œì»¬ì—ì„œëŠ” base64 ì‚¬ìš©
                        "prompt": prompt,
                        "backend": "local_gpu",
                        "generation_time": result.get("metadata", {}).get(
                            "generation_time", 0
                        ),
                        "metadata": result.get("metadata", {}),
                    }
                else:
                    return {
                        "success": False,
                        "error": "No image generated",
                        "backend": "local_gpu",
                    }
            else:
                return {
                    "success": False,
                    "error": result.get("error", "Unknown error"),
                    "backend": "local_gpu",
                }

        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ GPU ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": f"Local GPU generation failed: {str(e)}",
                "backend": "local_gpu",
                "retry_recommended": True,
            }

    def get_status(self) -> Dict[str, Any]:
        """ë¡œì»¬ GPU ìƒíƒœ í™•ì¸"""
        try:
            import torch

            status = {
                "backend": "local_gpu",
                "available": True,
                "model_path": self.model_path,
                "initialized": self.is_initialized,
                "cuda_available": torch.cuda.is_available(),
                "mps_available": hasattr(torch.backends, "mps")
                and torch.backends.mps.is_available(),
            }

            if torch.cuda.is_available():
                status["gpu_info"] = {
                    "device_name": torch.cuda.get_device_name(0),
                    "memory_allocated": torch.cuda.memory_allocated(0),
                    "memory_reserved": torch.cuda.memory_reserved(0),
                }
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                status["device"] = "Apple Silicon MPS"
            else:
                status["device"] = "CPU"
                status["warning"] = "GPU not available, using CPU (slow)"

            return status

        except ImportError:
            return {
                "backend": "local_gpu",
                "available": False,
                "error": "PyTorch not installed",
            }
        except Exception as e:
            return {"backend": "local_gpu", "available": False, "error": str(e)}


class RemoteGPUStrategy(ImageGenerationStrategy):
    """ì™¸ë¶€ GPU ì„œë²„ API í˜¸ì¶œ"""

    def __init__(self):
        self.api_url = settings.remote_gpu_url
        self.api_token = settings.remote_gpu_token
        self.timeout = 120.0  # 2ë¶„ íƒ€ì„ì•„ì›ƒ

        logger.info(f"ğŸŒ RemoteGPU ì „ëµ ì´ˆê¸°í™” (URL: {self.api_url})")

    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ì™¸ë¶€ GPU ì„œë²„ë¡œ ì´ë¯¸ì§€ ìƒì„±"""
        if not self.api_url or not self.api_token:
            return {
                "success": False,
                "error": "Remote GPU settings not configured",
                "backend": "remote_gpu",
            }

        try:
            generation_params = {
                "prompt": prompt,
                "width": kwargs.get("width", 512),
                "height": kwargs.get("height", 512),
                "num_inference_steps": kwargs.get("num_inference_steps", 20),
                "guidance_scale": kwargs.get("guidance_scale", 7.5),
                "seed": kwargs.get("seed", None),
            }

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.api_url}/generate",
                    json=generation_params,
                    headers={"Authorization": f"Bearer {self.api_token}"},
                )

                if response.status_code == 200:
                    result = response.json()

                    logger.info(f"âœ… ì›ê²© GPU ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {prompt[:50]}...")

                    return {
                        "success": True,
                        "image_b64": result.get("image_b64"),
                        "image_url": result.get("image_url"),
                        "prompt": prompt,
                        "backend": "remote_gpu",
                        "generation_time": result.get("generation_time", 0),
                        "metadata": result.get("metadata", {}),
                    }

                else:
                    logger.error(f"âŒ ì›ê²© GPU API ì˜¤ë¥˜: {response.status_code}")
                    return {
                        "success": False,
                        "error": f"Remote API error: {response.status_code} - {response.text}",
                        "backend": "remote_gpu",
                        "retry_recommended": response.status_code >= 500,
                    }

        except httpx.TimeoutException:
            logger.error("âŒ ì›ê²© GPU API íƒ€ì„ì•„ì›ƒ")
            return {
                "success": False,
                "error": "Remote API timeout",
                "backend": "remote_gpu",
                "retry_recommended": True,
            }

        except Exception as e:
            logger.error(f"âŒ ì›ê²© GPU ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": f"Remote GPU generation failed: {str(e)}",
                "backend": "remote_gpu",
                "retry_recommended": True,
            }

    def get_status(self) -> Dict[str, Any]:
        """ì›ê²© GPU ìƒíƒœ í™•ì¸"""
        if not self.api_url or not self.api_token:
            return {
                "backend": "remote_gpu",
                "available": False,
                "error": "Remote GPU settings not configured",
            }

        # ì‹¤ì œë¡œëŠ” /health ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œí•´ì•¼ í•¨
        return {
            "backend": "remote_gpu",
            "available": True,
            "api_url": self.api_url,
            "configured": bool(self.api_token),
            "note": "Status check requires actual API call",
        }


class ColabStrategy(ImageGenerationStrategy):
    """Google Colab ëŸ°íƒ€ì„ í™œìš©"""

    def __init__(self):
        self.notebook_url = settings.colab_notebook_url
        self.access_token = settings.colab_access_token
        self.timeout = 180.0  # 3ë¶„ íƒ€ì„ì•„ì›ƒ

        logger.info(f"ğŸ“” Colab ì „ëµ ì´ˆê¸°í™” (URL: {self.notebook_url})")

    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Google Colabì„ í†µí•œ ì´ë¯¸ì§€ ìƒì„±"""
        if not self.notebook_url:
            return {
                "success": False,
                "error": "Colab settings not configured",
                "backend": "colab",
            }

        try:
            # Colab ë…¸íŠ¸ë¶ê³¼ í†µì‹ í•˜ëŠ” ë¡œì§
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ngrok URLì´ë‚˜ Colab API í™œìš©

            generation_params = {
                "prompt": prompt,
                "width": kwargs.get("width", 512),
                "height": kwargs.get("height", 512),
                "num_inference_steps": kwargs.get("num_inference_steps", 20),
                "guidance_scale": kwargs.get("guidance_scale", 7.5),
            }

            # ì„ì‹œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” Colab ëŸ°íƒ€ì„ê³¼ í†µì‹ 
            logger.warning("âš ï¸  Colab í†µí•©ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

            return {
                "success": False,
                "error": "Colab integration not implemented yet",
                "backend": "colab",
                "note": "Colab integration requires ngrok tunnel or direct API",
            }

        except Exception as e:
            logger.error(f"âŒ Colab ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": f"Colab generation failed: {str(e)}",
                "backend": "colab",
            }

    def get_status(self) -> Dict[str, Any]:
        """Colab ìƒíƒœ í™•ì¸"""
        return {
            "backend": "colab",
            "available": False,
            "notebook_url": self.notebook_url,
            "configured": bool(self.notebook_url),
            "note": "Colab integration not implemented yet",
        }


class ImageService:
    """ì´ë¯¸ì§€ ìƒì„± ì„œë¹„ìŠ¤ (Strategy Pattern)"""

    def __init__(self):
        self.strategy = self._get_strategy()
        self.supported_backends = ["local", "remote", "colab"]

        logger.info(f"ğŸ¨ ì´ë¯¸ì§€ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” - ë°±ì—”ë“œ: {settings.image_backend}")

    def _get_strategy(self) -> ImageGenerationStrategy:
        """ì„¤ì •ì— ë”°ë¥¸ ì „ëµ ì„ íƒ"""
        backend = settings.image_backend.lower()

        if backend == "local":
            return LocalGPUStrategy()
        elif backend == "remote":
            return RemoteGPUStrategy()
        elif backend == "colab":
            return ColabStrategy()
        else:
            logger.warning(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ ë°±ì—”ë“œ: {backend}, localë¡œ ëŒ€ì²´")
            return LocalGPUStrategy()

    async def generate_image(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ìƒì„± (ë¹„ë™ê¸°)"""
        start_time = datetime.utcnow()

        try:
            # í”„ë¡¬í”„íŠ¸ ìœ íš¨ì„± ê²€ì¦
            if not prompt or not prompt.strip():
                return {
                    "success": False,
                    "error": "Empty prompt provided",
                    "backend": settings.image_backend,
                }

            # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ
            if len(prompt) > 500:
                prompt = prompt[:500]
                logger.warning("âš ï¸  í”„ë¡¬í”„íŠ¸ê°€ 500ìë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤")

            # ì´ë¯¸ì§€ ìƒì„±
            result = await self.strategy.generate(prompt, **kwargs)

            # ìƒì„± ì‹œê°„ ê³„ì‚°
            generation_time = (datetime.utcnow() - start_time).total_seconds()

            if "generation_time" not in result:
                result["generation_time"] = generation_time

            result["requested_at"] = start_time.isoformat()
            result["completed_at"] = datetime.utcnow().isoformat()

            if result["success"]:
                logger.info(
                    f"âœ… ì´ë¯¸ì§€ ìƒì„± ì„±ê³µ ({settings.image_backend}): {generation_time:.2f}s"
                )
            else:
                logger.error(
                    f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ ({settings.image_backend}): {result.get('error')}"
                )

            return result

        except Exception as e:
            generation_time = (datetime.utcnow() - start_time).total_seconds()

            logger.error(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")

            return {
                "success": False,
                "error": f"Image service error: {str(e)}",
                "backend": settings.image_backend,
                "generation_time": generation_time,
                "retry_recommended": True,
            }

    def get_backend_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ë°±ì—”ë“œ ìƒíƒœ í™•ì¸"""
        try:
            status = self.strategy.get_status()
            status.update(
                {
                    "service_info": {
                        "current_backend": settings.image_backend,
                        "supported_backends": self.supported_backends,
                        "service_status": "operational",
                    },
                    "timestamp": datetime.utcnow().isoformat(),
                }
            )

            return status

        except Exception as e:
            logger.error(f"âŒ ë°±ì—”ë“œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {
                "backend": settings.image_backend,
                "available": False,
                "error": str(e),
                "service_info": {
                    "current_backend": settings.image_backend,
                    "supported_backends": self.supported_backends,
                    "service_status": "error",
                },
                "timestamp": datetime.utcnow().isoformat(),
            }

    def switch_backend(self, new_backend: str) -> Dict[str, Any]:
        """ë°±ì—”ë“œ ë™ì  ë³€ê²½ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
        if new_backend not in self.supported_backends:
            return {
                "success": False,
                "error": f"Unsupported backend: {new_backend}",
                "supported": self.supported_backends,
            }

        try:
            old_backend = settings.image_backend
            settings.image_backend = new_backend
            self.strategy = self._get_strategy()

            logger.info(f"ğŸ”„ ì´ë¯¸ì§€ ë°±ì—”ë“œ ë³€ê²½: {old_backend} â†’ {new_backend}")

            return {
                "success": True,
                "old_backend": old_backend,
                "new_backend": new_backend,
                "timestamp": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            logger.error(f"âŒ ë°±ì—”ë“œ ë³€ê²½ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def batch_generate(self, prompts: list, **kwargs) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„±"""
        if len(prompts) > 10:
            logger.warning("âš ï¸  ë°°ì¹˜ ìƒì„±ì€ ìµœëŒ€ 10ê°œê¹Œì§€ ì§€ì›ë©ë‹ˆë‹¤")
            prompts = prompts[:10]

        results = []

        for i, prompt in enumerate(prompts):
            logger.info(f"ğŸ¨ ë°°ì¹˜ ìƒì„± ì¤‘ ({i+1}/{len(prompts)}): {prompt[:30]}...")
            result = await self.generate_image(prompt, **kwargs)
            result["batch_index"] = i
            results.append(result)

            # ì—°ì† ìƒì„± ê°„ ì§§ì€ ëŒ€ê¸° (ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€)
            if i < len(prompts) - 1:
                await asyncio.sleep(1)

        successful = sum(1 for r in results if r["success"])
        logger.info(f"âœ… ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {successful}/{len(prompts)} ì„±ê³µ")

        return results
