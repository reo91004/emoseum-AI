#!/usr/bin/env python3
"""
EmotionalImageTherapySystem - ê°ì • ê¸°ë°˜ ì´ë¯¸ì§€ ì¹˜ë£Œ ì‹œìŠ¤í…œ
"""

import os
import warnings
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union

import numpy as np
import torch
from PIL import Image

from config import (
    device,
    logger,
    TRANSFORMERS_AVAILABLE,
    DIFFUSERS_AVAILABLE,
    PEFT_AVAILABLE,
)
from models.emotion import EmotionEmbedding
from models.emotion_mapper import AdvancedEmotionMapper
from models.user_profile import UserEmotionProfile
from models.lora_manager import PersonalizedLoRAManager
from models.reward_model import DRaFTPlusRewardModel
from training.trainer import DRaFTPlusTrainer

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# ì„ íƒì  ì„í¬íŠ¸
if DIFFUSERS_AVAILABLE:
    from diffusers import (
        StableDiffusionPipeline,
        EulerDiscreteScheduler,
    )


class EmotionalImageTherapySystem:
    """ê°ì • ê¸°ë°˜ ì´ë¯¸ì§€ ì¹˜ë£Œ ì‹œìŠ¤í…œ"""

    def __init__(self, model_path: str = "runwayml/stable-diffusion-v1-5"):
        self.model_path = model_path
        self.device = device

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = Path("generated_images")
        self.output_dir.mkdir(exist_ok=True)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        logger.info("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")

        # 1. ê°ì • ë§¤í¼ ì´ˆê¸°í™”
        self.emotion_mapper = AdvancedEmotionMapper()

        # 2. LoRA ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.lora_manager = PersonalizedLoRAManager(model_path)

        # 3. SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        self.pipeline = self._load_pipeline()

        # 4. ë³´ìƒ ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        if self.pipeline:
            self.reward_model = DRaFTPlusRewardModel(self.device)
            self.trainer = DRaFTPlusTrainer(self.pipeline, self.reward_model)
        else:
            self.reward_model = None
            self.trainer = None

        # 5. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìºì‹œ
        self.user_profiles = {}

        logger.info("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")

    def _load_pipeline(self):
        """SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if not DIFFUSERS_AVAILABLE:
            logger.error("âŒ Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None

        try:
            logger.info(f"ğŸ“¦ Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘: {self.model_path}")

            pipeline = StableDiffusionPipeline.from_pretrained(
                self.model_path,
                torch_dtype=(
                    # ìœ íš¨í•˜ì§€ ì•Šì€ ìˆ«ìê°€ ë“¤ì–´ê°€ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë¯€ë¡œ ëª¨ë‘ float32ë¡œ ì„¤ì •
                    torch.float32
                    if self.device.type == "mps"
                    else torch.float32
                ),
                use_safetensors=True,
                safety_checker=None,  # ë¹ ë¥¸ ìƒì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
                requires_safety_checker=False,
            )

            # ìµœì í™” ì„¤ì •
            pipeline = pipeline.to(self.device)

            # ë©”ëª¨ë¦¬ ìµœì í™”
            pipeline.enable_attention_slicing()

            if self.device.type == "cuda":
                pipeline.enable_sequential_cpu_offload()

            # ë¹ ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ë³€ê²½
            pipeline.scheduler = EulerDiscreteScheduler.from_config(
                pipeline.scheduler.config
            )

            logger.info("âœ… SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ ë° ìµœì í™” ì™„ë£Œ")
            return pipeline

        except Exception as e:
            logger.error(f"âŒ SD íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_user_profile(self, user_id: str) -> UserEmotionProfile:
        """ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserEmotionProfile(user_id)
            logger.info(f"âœ… ìƒˆ ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìƒì„±: {user_id}")
        return self.user_profiles[user_id]

    def generate_therapeutic_image(
        self,
        user_id: str,
        input_text: str,
        base_prompt: str = "",
        num_inference_steps: int = 15,
        guidance_scale: float = 7.5,
        width: int = 512,
        height: int = 512,
    ) -> Dict[str, Any]:
        """ì¹˜ë£Œìš© ì´ë¯¸ì§€ ìƒì„±"""

        try:
            logger.info(f"ğŸ¨ ì‚¬ìš©ì {user_id}ì˜ ì´ë¯¸ì§€ ìƒì„± ì‹œì‘")
            logger.info(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {input_text}")

            # 1. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ë¡œë“œ
            user_profile = self.get_user_profile(user_id)

            # 2. ê°ì • ë¶„ì„
            emotion = self.emotion_mapper.extract_emotion_from_text(input_text)
            logger.info(
                f"ğŸ˜Š ê°ì • ë¶„ì„: V={emotion.valence:.3f}, A={emotion.arousal:.3f}, D={emotion.dominance:.3f}"
            )

            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            emotion_modifiers = self.emotion_mapper.emotion_to_prompt_modifiers(emotion)
            personal_modifiers = user_profile.get_personalized_style_modifiers()

            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not base_prompt:
                base_prompt = "digital art, beautiful scene"

            final_prompt = f"{base_prompt}, {emotion_modifiers}, {personal_modifiers}"
            final_prompt += ", high quality, detailed, masterpiece"

            logger.info(f"ğŸ¯ ìµœì¢… í”„ë¡¬í”„íŠ¸: {final_prompt}")

            # 4. ì´ë¯¸ì§€ ìƒì„±
            if self.pipeline:
                # SD íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
                with torch.autocast(
                    self.device.type if self.device.type != "mps" else "cpu"
                ):
                    result = self.pipeline(
                        prompt=final_prompt,
                        num_inference_steps=num_inference_steps,
                        guidance_scale=guidance_scale,
                        width=width,
                        height=height,
                        output_type="pil",
                    )

                generated_image = result.images[0]
                logger.info("âœ… SD íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
            else:
                # í´ë°±: ê°„ë‹¨í•œ ì´ë¯¸ì§€ ìƒì„±
                generated_image = self._generate_fallback_image(emotion, width, height)
                logger.info("âš ï¸ í´ë°± ì´ë¯¸ì§€ ìƒì„±ê¸° ì‚¬ìš©")

            # 5. ì´ë¯¸ì§€ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            image_filename = f"{user_id}_{timestamp}.png"
            image_path = self.output_dir / image_filename
            generated_image.save(image_path)

            # 6. ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ë¡
            emotion_id = user_profile.add_emotion_record(
                input_text=input_text,
                emotion=emotion,
                generated_prompt=final_prompt,
                image_path=str(image_path),
            )

            # 7. ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "emotion_id": emotion_id,
                "user_id": user_id,
                "input_text": input_text,
                "emotion": emotion.to_dict(),
                "final_prompt": final_prompt,
                "image_path": str(image_path),
                "image_filename": image_filename,
                "generation_params": {
                    "num_inference_steps": num_inference_steps,
                    "guidance_scale": guidance_scale,
                    "width": width,
                    "height": height,
                },
                "timestamp": timestamp,
                "device": str(self.device),
            }

            logger.info(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {image_path}")
            return {"success": True, "image": generated_image, "metadata": metadata}

        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metadata": {"user_id": user_id, "input_text": input_text},
            }

    def _generate_fallback_image(
        self, emotion: EmotionEmbedding, width: int = 512, height: int = 512
    ) -> Image.Image:
        """í´ë°± ì´ë¯¸ì§€ ìƒì„± (SD íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ì‹œ)"""

        # ê°ì • ê¸°ë°˜ ìƒ‰ìƒ ìƒì„±
        if emotion.valence > 0.3:
            # ê¸ì •ì  ê°ì • - ë”°ëœ»í•œ ìƒ‰ìƒ
            base_color = [0.9, 0.8, 0.6]  # ë”°ëœ»í•œ ë…¸ë€ìƒ‰
        elif emotion.valence < -0.3:
            # ë¶€ì •ì  ê°ì • - ì°¨ê°€ìš´ ìƒ‰ìƒ
            base_color = [0.6, 0.7, 0.9]  # ì°¨ê°€ìš´ íŒŒë€ìƒ‰
        else:
            # ì¤‘ì„± ê°ì • - ì¤‘ê°„ ìƒ‰ìƒ
            base_color = [0.7, 0.7, 0.8]  # íšŒìƒ‰ë¹›

        # ê°ì„±ë„ ê¸°ë°˜ ê°•ë„ ì¡°ì •
        intensity = 0.5 + abs(emotion.arousal) * 0.5
        base_color = [c * intensity for c in base_color]

        # ê·¸ë¼ë°ì´ì…˜ ì´ë¯¸ì§€ ìƒì„±
        image_array = np.zeros((height, width, 3))

        for i in range(height):
            for j in range(width):
                # ì¤‘ì‹¬ì—ì„œì˜ ê±°ë¦¬ ê¸°ë°˜ ê·¸ë¼ë°ì´ì…˜
                center_x, center_y = width // 2, height // 2
                distance = np.sqrt((j - center_x) ** 2 + (i - center_y) ** 2)
                max_distance = np.sqrt(center_x**2 + center_y**2)

                # ê°ì • ê¸°ë°˜ ê·¸ë¼ë°ì´ì…˜ íŒ¨í„´
                if emotion.dominance > 0:
                    # ì§€ë°°ì  ê°ì • - ì¤‘ì‹¬ì—ì„œ ë°”ê¹¥ìœ¼ë¡œ
                    factor = 1.0 - (distance / max_distance) * 0.5
                else:
                    # ìˆ˜ë™ì  ê°ì • - ë°”ê¹¥ì—ì„œ ì¤‘ì‹¬ìœ¼ë¡œ
                    factor = 0.5 + (distance / max_distance) * 0.5

                image_array[i, j] = [c * factor for c in base_color]

        # numpy ë°°ì—´ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        image_array = np.clip(image_array * 255, 0, 255).astype(np.uint8)
        return Image.fromarray(image_array)

    def process_feedback(
        self,
        user_id: str,
        emotion_id: int,
        feedback_score: float,
        feedback_type: str = "rating",
        comments: str = None,
        enable_training: bool = True,
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”¼ë“œë°± ì²˜ë¦¬ ë° ê°œì¸í™” í•™ìŠµ"""

        try:
            logger.info(f"ğŸ“ ì‚¬ìš©ì {user_id} í”¼ë“œë°± ì²˜ë¦¬: ì ìˆ˜ {feedback_score}")

            # 1. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ë¡œë“œ
            user_profile = self.get_user_profile(user_id)

            # 2. í”¼ë“œë°± ì €ì¥
            user_profile.add_feedback(
                emotion_id=emotion_id,
                feedback_score=feedback_score,
                feedback_type=feedback_type,
                comments=comments,
            )

            # 3. ê°•í™”í•™ìŠµ ìˆ˜í–‰ (ì˜µì…˜)
            training_result = None
            if (
                enable_training and self.trainer and feedback_score != 3.0
            ):  # ì¤‘ì„± í”¼ë“œë°± ì œì™¸

                # í•´ë‹¹ ê°ì • ê¸°ë¡ ì°¾ê¸°
                emotion_record = None
                for record in user_profile.emotion_history:
                    if record.get("id") == emotion_id:
                        emotion_record = record
                        break

                if emotion_record:
                    logger.info("ğŸ¤– ê°œì¸í™” í•™ìŠµ ì‹œì‘...")
                    training_result = self.trainer.train_step(
                        prompt=emotion_record["generated_prompt"],
                        target_emotion=emotion_record["emotion"],
                        user_profile=user_profile,
                        num_inference_steps=8,  # ë¹ ë¥¸ í•™ìŠµ
                    )
                    logger.info(
                        f"âœ… í•™ìŠµ ì™„ë£Œ: ë³´ìƒ {training_result.get('total_reward', 0):.3f}"
                    )

            # 4. LoRA ì–´ëŒ‘í„° ì €ì¥ (ì£¼ê¸°ì )
            if len(user_profile.feedback_history) % 5 == 0:  # 5ë²ˆì§¸ í”¼ë“œë°±ë§ˆë‹¤ ì €ì¥
                self._save_user_lora_if_needed(user_id, user_profile)

            # 5. ì¹˜ë£Œ ì¸ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸
            insights = user_profile.get_therapeutic_insights()

            result = {
                "success": True,
                "feedback_recorded": True,
                "training_performed": training_result is not None,
                "training_result": training_result,
                "therapeutic_insights": insights,
                "total_interactions": len(user_profile.emotion_history),
                "total_feedbacks": len(user_profile.feedback_history),
            }

            logger.info("âœ… í”¼ë“œë°± ì²˜ë¦¬ ì™„ë£Œ")
            return result

        except Exception as e:
            logger.error(f"âŒ í”¼ë“œë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "feedback_recorded": False,
                "training_performed": False,
            }

    def _save_user_lora_if_needed(self, user_id: str, user_profile: UserEmotionProfile):
        """í•„ìš”ì‹œ ì‚¬ìš©ì LoRA ì–´ëŒ‘í„° ì €ì¥"""
        try:
            if self.pipeline and hasattr(self.pipeline, "unet"):
                # í˜„ì¬ ëª¨ë¸ ìƒíƒœë¥¼ LoRAë¡œ ì €ì¥
                model_state = {
                    "unet_state_dict": self.pipeline.unet.state_dict(),
                    "user_preferences": user_profile.preference_weights,
                    "training_metadata": user_profile.learning_metadata,
                }

                self.lora_manager.save_user_lora(user_id, model_state)
                logger.info(f"ğŸ’¾ ì‚¬ìš©ì {user_id} LoRA ì–´ëŒ‘í„° ì €ì¥")
        except Exception as e:
            logger.warning(f"âš ï¸ LoRA ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_user_insights(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì¹˜ë£Œ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        user_profile = self.get_user_profile(user_id)
        return user_profile.get_therapeutic_insights()

    def get_emotion_history(self, user_id: str, limit: int = 10) -> List[Dict]:
        """ì‚¬ìš©ì ê°ì • íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        user_profile = self.get_user_profile(user_id)
        return user_profile.emotion_history[-limit:]

    def cleanup_old_images(self, days_old: int = 30):
        """ì˜¤ë˜ëœ ì´ë¯¸ì§€ íŒŒì¼ ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_old)
            cleaned_count = 0

            for image_file in self.output_dir.glob("*.png"):
                if image_file.stat().st_mtime < cutoff_date.timestamp():
                    image_file.unlink()
                    cleaned_count += 1

            logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ì´ë¯¸ì§€ {cleaned_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
            return cleaned_count

        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0